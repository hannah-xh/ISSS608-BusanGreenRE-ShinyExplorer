[
  {
    "objectID": "Teams/Proposal/proposal.html",
    "href": "Teams/Proposal/proposal.html",
    "title": "Green Spaces and Real Estate: Exploring the Influence of Environmental Factors on Housing Prices in Busan",
    "section": "",
    "text": "Generate by ChatGPT\n\n\n\n1 Background\nIn recent years, there has been a growing awareness of the benefits of living near environmental amenities, as more research uncovers the impact of neighborhood environments on both physical and mental health[1][2][3][4].\nStudies have shown that factors such as local built environment, proximity to environmental and built amenities and neighbourhood demographics, can significantly influence well-being. Specifically, residing near green spaces has been found to have a positive effect on mental health and encourages physical activity. Correspondingly, some studies have also found that environmental amenities could play a key role in influencing property prices [4], with growing recognition of the importance of green spaces. In Korea for instance, there is a gradual shift towards preference for residential areas near green spaces [5]. However, few studies have examined the relationship between proximity to green amenities and housing prices in Korea.¬†\n\n\n2 Objectives\nThe primary objective of this project is to investigate how various factors, with a greater focus on the environmental amenities and the built environment factors, influences property prices in Korea. The project seeks to understand whether properties located closer to green spaces and other amenities tend to have higher market values. The secondary objective is to explore the interaction between green amenities, the built environment, and other factors, such as neighborhood demographics and seasonality, in shaping property prices.\nThe results from this project will help us gain insight on the factors driving property values in urban settings. As urbanization continues, the demand for green spaces and well-planned environments will likely increase. Understanding the connection between environmental amenities and local built environment and property prices could aid in creating more sustainable and livable urban areas, by offering insights for policymakers, estate developers and urban planners to make data-driven decisions on developing the city‚Äôs infrastructure and green spaces.\n\n\n3 Dataset\nTo achieve our objectives, we obtained a dataset used in a research paper to investigate variables influencing property prices in Korea. The dataset comprises of comprehensive information on the property prices in the Busan Metropolitan City of South Korea for transactions in 2018 to 2019 and various factors that influence property prices including characteristics of the property, distance to environmental amenities and local built environments, local demographic characteristics, and season in which the transaction occurred.¬†\n\n\n4 Methodology & Approach\nWe will analyse and visualise the relationships between different factors and the housing prices in Busan by creating interactive R shiny applications.¬† Users will also be able to analyse the data based on their interests and draw their own insights and conclusions regarding the housing prices in Busan.\nWe will create 4 sub-modules to enable univariate analyses, bivariate analyses and multivariate analyses.\n\n\n5 Prototype Sketches\nThe following sketches were drawn to conceptualise our thoughts and discussions on the layout of the Shiny app along with its various functions.¬†\n\n5.1 Univariate Analysis5.2 Bivariate Analysis5.3 Multivariate Analysis\n\n\nIn this sub-module, we aim to provide tools for analyzing individual variables. The interface includes summary statistics (mean, median, min, max, standard deviation), controls for variable selection and filtering, and a distribution visualization combining boxplot and half eye plot to display the selected variable‚Äôs characteristics.¬†\n\n\n\nThis sub-module provides tools for examining relationships between pairs of variables. The interface features a correlation heatmap on the left showing strength of relationships between multiple variables, and a scatterplot on the right displaying the relationship between two selected variables. Controls allow users to select which continuous variables to include in the analysis for both visualization methods.¬†\n\n\n\nThis panel offers tools for analyzing multiple variables simultaneously. It includes controls for selecting variables for clustering analysis, a dendrogram visualization showing hierarchical clustering results, and a parallel coordinate plot displaying relationships across multiple dimensions.¬†\n\nThis panel provides radar plot functionality for multivariate visualization. Controls allow users to select encoded continuous variables and filter by a categorical variable. The radar plot displays multiple dimensions simultaneously with different colors representing different categories.\n\n\n\n\n\n\n6 R packages\nThe R packages we will be using for the project include:¬†\n\nTidyverse for manipulating the data¬†\n\n\n\nggstatplot for visualising statistcal analysis¬†\n\n\n\nGGally / ggpcp / ggparallel for creating parallel coordinate plot¬†\n\n\n\nggdist for visualising distributions\n\n\n\ncorrplot for correlation heatmap¬†\n\n\n\nGgESDA / ggradar / fmsb for radar chart¬†\n\n\n\n7 Project Timeline\n\n\n\n8 Reference\n[1]Yim, D. H., & Kwon, Y. (2021). Does young adults‚Äô neighborhood environment affect their depressive mood? Insights from the 2019 Korean community health survey.¬†International Journal of Environmental Research and Public Health,¬†18(3), 1269.\n[2]Choi, K. A., & Rezaei, M. (2022). Assessing the Correlation between Neighborhood Green Areas and the Perceived Mental Health of Residents in Metropolitan Areas.¬†Iranian journal of public health,¬†51(9), 2027.\n\n[3]Han, M. J. N., & Kim, M. J. (2019). Green environments and happiness level in housing areas toward a sustainable life.¬†Sustainability,¬†11(17), 4768.\n[4]An, S., Jang, H., Kim, H., Song, Y., & Ahn, K. (2023). Assessment of street-level greenness and its association with housing prices in a metropolitan area.¬†Scientific reports,¬†13(1), 22577.\n[5]Korea Bizwire. (2023, July 8). Changing apartment preferences: Green spaces and comfort take priority in Korea‚Äôs housing market."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes3.html",
    "href": "Teams/Minutes/Meeting_Minutes3.html",
    "title": "Project Meeting 3: Project Proposal 2",
    "section": "",
    "text": "Info\n\n\n\nAttendance: Moo Jia Rong, Zhang Xiao Han, Chen Peng-Wei\nDate: 22/03/2025 5.00pm ‚Äì 6.00pm\nMeeting Agenda:\n\nProject Modules & Shiny app Task allocations (updated base on prof reply)\nFollow-up Action"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes3.html#agenda-item-1-project-modules-shiny-app-task-allocations",
    "href": "Teams/Minutes/Meeting_Minutes3.html#agenda-item-1-project-modules-shiny-app-task-allocations",
    "title": "Project Meeting 3: Project Proposal 2",
    "section": "Agenda Item 1: Project Modules & Shiny app Task allocations",
    "text": "Agenda Item 1: Project Modules & Shiny app Task allocations\nFollowing Prof‚Äôs feedback on our proposal, the team decided to re-consider the modules to be created for the project. Prof recommended the team to split the modules according to (1) EDA & CDA, (2) Explanatory Modelling and (3) Predictive Modelling.¬†¬†\nJia Rong said that the modules the team has previously proposed are all on EDA & CDA. Hence the team should select a few of the previous ideas and work on it as 1 module for EDA & CDA. For example, the team could do the boxplot and smoothed histogram for EDA, the scatterplot and the correlation heatmap as CDA. In addition, boxplot for ANOVA can be added for CDA.¬†\nFor explanatory modelling, Jia Rong suggested doing multiple linear regression and perhaps latent class analysis, with reference to senior‚Äôs work. As for predictive modelling, Jia Rong suggested trying out random forest and decision tree.¬†\nPeng-Wei suggested that for the random forest part, we can provide an interactive adjustment interface for parameters (such as the number of trees , the number of variables), and instantly displays the model‚Äôs prediction performance (such as Accuracy, RMSE, MAE, etc.) in the Shiny app. As for the decision Tree, we can make user adjust parameters such as tree depth and splitting method on the interface, and displaying auxiliary analysis charts such as predicted vs.¬†actual values, residual plots, etc.¬†\nXiao Han suggested using tidyverse, car, and lmtest packages for multiple linear regression analysis with ggplot2 for regression diagnostic plots, while implementing poLCA, mclust, and flexmix packages for latent class analysis since you have previous experience with this technique. She also recommended creating interactive elements using shiny, shinydashboard, and plotly to allow users to dynamically select variables and adjust model parameters."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes3.html#agenda-item-2-follow-up-action",
    "href": "Teams/Minutes/Meeting_Minutes3.html#agenda-item-2-follow-up-action",
    "title": "Project Meeting 3: Project Proposal 2",
    "section": "Agenda Item 2: Follow-up Action",
    "text": "Agenda Item 2: Follow-up Action\nJia Rong will work on the EDA + CDA submodule.¬†\nPeng-Wei will work on the predictive modelling module.¬†\nXiao Han will work on the explanatory modelling module.¬†\nAll team members agreed to finish their modules by 29th March and have another meeting to work on the poster and the user guide."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes1.html",
    "href": "Teams/Minutes/Meeting_Minutes1.html",
    "title": "Project Meeting 1: Project Dataset, Project Proposal and Project Timeline",
    "section": "",
    "text": "Info\n\n\n\nAttendance: Chen.PengWei, Moo.JiaRong, Zhang.XiaoHan\nDate: 23/02/2025 4.00pm ‚Äì 05.30pm\nMeeting Agenda:\n\nProject Topic/Dataset\n\nProject Timeline\n\nProject Proposal\n\nWebsite Design Method\n\nFollow-up Action"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-1-project-topicdataset",
    "href": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-1-project-topicdataset",
    "title": "Project Meeting 1: Project Dataset, Project Proposal and Project Timeline",
    "section": "Agenda Item 1: Project Topic/Dataset",
    "text": "Agenda Item 1: Project Topic/Dataset\nPeng-Wei suggested using a traffic dataset from Taiwan, which has details on the weather and location of the accidents.¬† However, as the dataset is in Chinese, the group decided that it will take too much time to encode the data in English.¬†\nXiao Han suggested working on a dataset about problematic internet use in children. However, this dataset requires a lot of encoding and might be more suitable for machine learning.¬†\nJia Rong suggested using the Seoul Bike Sharing Demand dataset and conducting EDA and a prediction model on it. However, the dataset is a bit old (2017‚Äì2018).¬†\nXiaohan mentioned that the global traffic accident dataset could be used, but Jia Rong thinks that the dataset has too few feature columns. However, Peng-wei and Xiaohan believe that this dataset is more suitable for visualization as it includes latitude and longitude information.¬†\nIn the end, all three agreed to choose the Seoul bicycle dataset and the global traffic accident dataset as alternatives and will consult the professor later."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-2-project-timeline",
    "href": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-2-project-timeline",
    "title": "Project Meeting 1: Project Dataset, Project Proposal and Project Timeline",
    "section": "Agenda Item 2: Project Timeline¬†",
    "text": "Agenda Item 2: Project Timeline¬†\nThis timeline outlines key tasks, including proposal drafting, data cleaning, Shiny module development, poster creation, and user guide completion, ensuring a structured workflow."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-3-project-proposal",
    "href": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-3-project-proposal",
    "title": "Project Meeting 1: Project Dataset, Project Proposal and Project Timeline",
    "section": "Agenda Item 3: Project Proposal",
    "text": "Agenda Item 3: Project Proposal\n\nBelow is the proposal structure that we have agreed on:¬†\n(1) Motivations¬†¬†\n(2) Objective¬†¬†\n(3) Dataset description¬†¬†\n(4) Method ‚Äì Data analysis approaches and the 3 different Shiny sub-modules each member will work on¬†¬†\n(5) Prototype/storyboard drawing¬†¬†\n(6) List of R packages that we will use¬†¬†¬†\n(7) Project timeline in Gantt Chart"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-4-website-design-method",
    "href": "Teams/Minutes/Meeting_Minutes1.html#agenda-item-4-website-design-method",
    "title": "Project Meeting 1: Project Dataset, Project Proposal and Project Timeline",
    "section": "Agenda Item 4: Website design method",
    "text": "Agenda Item 4: Website design method\nFor the presentation style of the website, we want it to look professional and visually appealing, but without overly cute elements, as our content is related to transportation.¬†\nOur initial idea is to place relevant images on the homepage and design a professional and aesthetically pleasing logo.¬†\n\nAgenda Item 5: Follow-up Action\nPeng-Wei will send the email to professor to check which dataset (Seoul Bike Sharing & Global traffic) is more suitable to use in this project after this meeting.¬†\n\n\n\n\n\n\n\nBefore the next meeting:¬†\n\n\n\nAll team members will work on brainstorming the analysis approach, the type of visualizations we can create using our selected dataset and the potential topic for our project."
  },
  {
    "objectID": "Teams/About_Us/About_Us.html",
    "href": "Teams/About_Us/About_Us.html",
    "title": "ü´† A Team Held Together by Deadlines",
    "section": "",
    "text": "When creativity runs dry, Shiny keeps us cry‚Ä¶"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3b.html#introduction",
    "href": "Prototype/Explanatory/Take-home_Ex3b.html#introduction",
    "title": "MLR Model design",
    "section": "1. Introduction",
    "text": "1. Introduction\nThis document presents a Multiple Linear Regression (MLR) analysis of property prices, examining various factors that influence real estate values. The analysis aims to identify key determinants of property prices and develop a predictive model.\n\n\nLoad required packages\n# Core packages\nlibrary(shiny)       # Shiny application framework\nlibrary(shinydashboard) # Dashboard interface\nlibrary(DT)          # Interactive data tables\nlibrary(ggplot2)     # Data visualization\nlibrary(dplyr)       # Data manipulation\nlibrary(tidyr)       # Data tidying\nlibrary(corrplot)    # Correlation matrix visualization\nlibrary(car)         # VIF calculation and regression diagnostics\nlibrary(GGally)      # Scatterplot matrices\nlibrary(MASS)        # Various statistical functions including BoxCox transformation\nlibrary(scales)      # Scale tools for improved visualization\nlibrary(plotly)      # Interactive charts\nlibrary(RColorBrewer) # Color schemes\nlibrary(readxl)      # Read Excel files\n\n\n\n\nLoad dataset\nProperty_data &lt;- read_excel(\"data/Property_Price_and_Green_Index.xlsx\")\n\nglimpse(Property_data)"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3b.html#data-preprocessing",
    "href": "Prototype/Explanatory/Take-home_Ex3b.html#data-preprocessing",
    "title": "MLR Model design",
    "section": "2. Data Preprocessing",
    "text": "2. Data Preprocessing\nThe dataset requires preprocessing to ensure it‚Äôs suitable for regression analysis, including handling missing values, converting categorical variables, and examining distributions.\n\n\nData preprocessing functions\n# Data preprocessing function\npreprocess_data &lt;- function(data) {\n  # Create a copy to avoid modifying the original\n  df &lt;- data %&gt;% as.data.frame()\n  \n  # Convert season variables to factors\n  df &lt;- df %&gt;%\n    mutate(\n      Spring = as.factor(Spring),\n      Fall = as.factor(Fall),\n      Winter = as.factor(Winter),\n      Heating = as.factor(Heating),\n      `Top Univ.` = as.factor(`Top Univ.`)\n    )\n  \n  # Check for missing values\n  missing_summary &lt;- colSums(is.na(df))\n  \n  # Handle missing values if any\n  if(any(missing_summary &gt; 0)) {\n    # Fill numeric variables with median\n    for(col in names(df)) {\n      if(is.numeric(df[[col]]) && sum(is.na(df[[col]])) &gt; 0) {\n        df[[col]][is.na(df[[col]])] &lt;- median(df[[col]], na.rm = TRUE)\n      }\n    }\n  }\n  \n  return(list(\n    data = df,\n    missing_summary = missing_summary\n  ))\n}\n\n# Function to create a visualization sample for faster plotting\ncreate_viz_sample &lt;- function(data, n = 10000) {\n  if(nrow(data) &gt; n) {\n    return(data[sample(nrow(data), n), ])\n  } else {\n    return(data)\n  }\n}\n\n# Assuming Property_data is already loaded\n# Process the data (using full dataset)\nprocessed &lt;- preprocess_data(Property_data)\ndata &lt;- processed$data\n\n# Create visualization sample\nviz_data &lt;- create_viz_sample(data)\n\n# Display missing values summary\ncat(\"Missing values summary:\\n\")\nif(sum(processed$missing_summary) == 0) {\n  cat(\"No missing values in the dataset.\\n\")\n} else {\n  print(processed$missing_summary[processed$missing_summary &gt; 0])\n}\n\n# Basic data summary (based on full dataset)\ncat(\"\\nData summary (full dataset):\\n\")\nsummary(data)"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3b.html#exploratory-data-analysis",
    "href": "Prototype/Explanatory/Take-home_Ex3b.html#exploratory-data-analysis",
    "title": "MLR Model design",
    "section": "3. Exploratory Data Analysis",
    "text": "3. Exploratory Data Analysis\nBefore building the model, we need to explore the distributions of key variables and identify potential issues.\n\n\nMissing values visualization\n# Plot missing values visualization\nmissing_plot &lt;- function(data) {\n  missing_data &lt;- data.frame(\n    variable = names(data),\n    count = colSums(is.na(data)),\n    percent = colSums(is.na(data)) / nrow(data) * 100\n  )\n  \n  ggplot(missing_data, aes(x = reorder(variable, -percent), y = percent)) +\n    geom_bar(stat = \"identity\", fill = \"steelblue\") +\n    geom_text(aes(label = sprintf(\"%.1f%%\", percent)), \n              vjust = -0.5, size = 3) +\n    theme_minimal() +\n    labs(title = \"Missing Values by Variable\",\n         x = \"Variables\",\n         y = \"Percentage (%)\") +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n}\n\n# Generate and display missing values plot (if any)\nif(sum(processed$missing_summary) &gt; 0) {\n  missing_viz &lt;- missing_plot(Property_data)  # Using full dataset for accurate percentages\n  print(missing_viz)\n}\n\n\n\n\nVariable distribution analysis\n# Function to visualize variable distributions\nplot_distribution &lt;- function(data, variable, full_data = NULL) {\n  var_data &lt;- data[[variable]]\n  \n  # Calculate statistics from full dataset if provided\n  if(!is.null(full_data)) {\n    full_var_data &lt;- full_data[[variable]]\n    mean_val &lt;- mean(full_var_data, na.rm = TRUE)\n    median_val &lt;- median(full_var_data, na.rm = TRUE)\n    skew_val &lt;- skewness(full_var_data, na.rm = TRUE)\n    kurt_val &lt;- kurtosis(full_var_data, na.rm = TRUE)\n  } else {\n    mean_val &lt;- mean(var_data, na.rm = TRUE)\n    median_val &lt;- median(var_data, na.rm = TRUE)\n    skew_val &lt;- skewness(var_data, na.rm = TRUE)\n    kurt_val &lt;- kurtosis(var_data, na.rm = TRUE)\n  }\n  \n  # Create histogram with density curve\n  ggplot(data.frame(value = var_data), aes(x = value)) +\n    geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"darkblue\", alpha = 0.7) +\n    geom_density(color = \"red\", linewidth = 1) +\n    geom_vline(xintercept = mean_val, color = \"darkgreen\", linetype = \"dashed\", linewidth = 1) +\n    geom_vline(xintercept = median_val, color = \"purple\", linetype = \"dashed\", linewidth = 1) +\n    theme_minimal() +\n    labs(title = paste(\"Distribution of\", variable),\n         subtitle = paste(\"Skewness:\", round(skew_val, 2), \"| Kurtosis:\", round(kurt_val, 2)),\n         x = variable,\n         y = \"Density\",\n         caption = paste(\"Mean (green):\", round(mean_val, 2), \"| Median (purple):\", round(median_val, 2),\n                         ifelse(!is.null(full_data), \"\\nStatistics calculated on full dataset\", \"\")))\n}\n\n# Visualize the distribution of target variable (Property Prices)\nprice_dist &lt;- plot_distribution(viz_data, \"Property Prices\", full_data = data)\nprint(price_dist)\n\n# Visualize distributions of numeric variables\nnumeric_vars &lt;- names(data)[sapply(data, is.numeric)]\nfor(var in numeric_vars[1:5]) {  # Show first 5 variables as example\n  dist_plot &lt;- plot_distribution(viz_data, var, full_data = data)\n  print(dist_plot)\n}"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3b.html#variable-transformation",
    "href": "Prototype/Explanatory/Take-home_Ex3b.html#variable-transformation",
    "title": "MLR Model design",
    "section": "4. Variable Transformation",
    "text": "4. Variable Transformation\nProperty prices typically follow a right-skewed distribution. Transforming the target variable can improve model performance by creating more linear relationships with predictors.\n\n\nTarget variable transformation\n# Simplified transform_variable function without complex Box-Cox\ntransform_variable &lt;- function(data, variable, method) {\n  # Get the data\n  var_data &lt;- data[[variable]]\n  \n  if(method == \"log\") {\n    # Log transformation\n    if(min(var_data, na.rm = TRUE) &lt;= 0) {\n      const &lt;- abs(min(var_data, na.rm = TRUE)) + 1\n      transformed &lt;- log(var_data + const)\n      name &lt;- paste0(\"log(\", variable, \" + \", const, \")\")\n    } else {\n      transformed &lt;- log(var_data)\n      name &lt;- paste0(\"log(\", variable, \")\")\n    }\n  } else if(method == \"sqrt\") {\n    # Square root transformation\n    if(min(var_data, na.rm = TRUE) &lt; 0) {\n      const &lt;- abs(min(var_data, na.rm = TRUE)) + 1\n      transformed &lt;- sqrt(var_data + const)\n      name &lt;- paste0(\"sqrt(\", variable, \" + \", const, \")\")\n    } else {\n      transformed &lt;- sqrt(var_data)\n      name &lt;- paste0(\"sqrt(\", variable, \")\")\n    }\n  } else if(method == \"boxcox\") {\n    # Simple Box-Cox approximation\n    if(min(var_data, na.rm = TRUE) &lt;= 0) {\n      # Handle non-positive values\n      const &lt;- abs(min(var_data, na.rm = TRUE)) + 1\n      temp_data &lt;- var_data + const\n      \n      # Use simple log as Box-Cox approximation\n      transformed &lt;- log(temp_data)\n      name &lt;- paste0(\"log(\", variable, \" + \", const, \")\")\n    } else {\n      # Use log as simplified Box-Cox\n      transformed &lt;- log(var_data)\n      name &lt;- paste0(\"log(\", variable, \")\")\n    }\n  } else {\n    # No transformation\n    transformed &lt;- var_data\n    name &lt;- variable\n  }\n  \n  return(list(\n    transformed = transformed,\n    name = name,\n    original = var_data\n  ))\n}\n\n# Calculate transformations on full dataset\ncat(\"\\nCalculating transformations on full dataset...\\n\")\ntransformations &lt;- c(\"original\", \"log\", \"sqrt\", \"boxcox\")\nprice_transforms &lt;- lapply(transformations, function(method) {\n  if(method == \"original\") {\n    return(list(\n      transformed = data$`Property Prices`,\n      name = \"Original\",\n      original = data$`Property Prices`\n    ))\n  } else {\n    return(transform_variable(data, \"Property Prices\", method))\n  }\n})\n\n# Create sample transforms for visualization\nviz_transforms &lt;- list()\nfor(i in seq_along(price_transforms)) {\n  # Get sample of original and transformed values for visualization\n  idx &lt;- sample(length(price_transforms[[i]]$original), \n                min(10000, length(price_transforms[[i]]$original)))\n  \n  viz_transforms[[i]] &lt;- list(\n    transformed = price_transforms[[i]]$transformed[idx],\n    name = price_transforms[[i]]$name,\n    original = price_transforms[[i]]$original[idx]\n  )\n}\n\n# Combine transformations into a data frame for plotting\ntransform_df &lt;- data.frame(\n  original = viz_transforms[[1]]$original\n)\n\nfor(i in seq_along(viz_transforms)) {\n  transform_df[[viz_transforms[[i]]$name]] &lt;- viz_transforms[[i]]$transformed\n}\n\n# Create comparison plots\ntransform_long &lt;- reshape2::melt(transform_df, id.vars = NULL)\n\nggplot(transform_long, aes(x = value, fill = variable)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ variable, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"Comparison of Transformations for Property Prices\",\n       subtitle = \"Based on sample data, transformations calculated on full dataset\",\n       x = \"Value\", y = \"Density\", fill = \"Transformation\")\n\n# Apply the best transformation (log in this case) to Property Prices\ncat(\"\\nApplying log transformation to full dataset...\\n\")\ndata$LogPrice &lt;- transform_variable(data, \"Property Prices\", \"log\")$transformed\n\n# Compare the original and transformed distributions using sample\nviz_compare &lt;- data.frame(\n  value = c(viz_data$`Property Prices`, \n            transform_variable(viz_data, \"Property Prices\", \"log\")$transformed),\n  type = factor(rep(c(\"Original\", \"Log-transformed\"), each = nrow(viz_data)))\n)\n\nggplot(viz_compare, aes(x = value, fill = type)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ type, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"Original vs. Log-transformed Property Prices\",\n       subtitle = \"Sample visualization, transformation parameters from full dataset\",\n       x = \"Value\", y = \"Density\", fill = \"\")\n\ncat(\"\\nData preparation complete. Full dataset has\", nrow(data), \"rows and\", ncol(data), \"columns.\\n\")\ncat(\"Log-transformed target variable added as 'LogPrice'.\\n\")"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3b.html#mlr-pre-check-functions",
    "href": "Prototype/Explanatory/Take-home_Ex3b.html#mlr-pre-check-functions",
    "title": "MLR Model design",
    "section": "5. MLR Pre-Check Functions",
    "text": "5. MLR Pre-Check Functions\nBefore building the regression model, several checks are necessary to ensure valid analysis, including multicollinearity assessment, outlier detection, and linearity verification.\n\n\nMLR pre-check setup\n# Load required packages for MLR pre-checks\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(car)\nlibrary(GGally)\nlibrary(gridExtra)\nlibrary(nortest)\n\n# Function to create a visualization sample for faster plotting\ncreate_viz_sample &lt;- function(data, n = 10000) {\n  if(nrow(data) &gt; n) {\n    return(data[sample(nrow(data), n), ])\n  } else {\n    return(data)\n  }\n}\n\n# Create visualization sample \nviz_data &lt;- create_viz_sample(data)\n\ncat(\"Starting MLR pre-check analysis...\\n\")\ncat(\"Full dataset has\", nrow(data), \"rows and\", ncol(data), \"columns.\\n\")\ncat(\"Visualization sample has\", nrow(viz_data), \"rows.\\n\")\n\n\n\n5.1 Multicollinearity Check\n\n\nMulticollinearity check functions\n# Function to check multicollinearity\ncheck_multicollinearity &lt;- function(data, threshold = 0.7) {\n  cat(\"Calculating correlation matrix on full dataset...\\n\")\n  \n  # Select only numeric variables\n  numeric_data &lt;- data %&gt;% select_if(is.numeric)\n  \n  # Calculate correlation matrix\n  cor_matrix &lt;- cor(numeric_data, use = \"pairwise.complete.obs\")\n  \n  # Find highly correlated variables\n  high_cor &lt;- which(abs(cor_matrix) &gt; threshold & abs(cor_matrix) &lt; 1, arr.ind = TRUE)\n  high_cor_pairs &lt;- data.frame(\n    var1 = rownames(cor_matrix)[high_cor[,1]],\n    var2 = colnames(cor_matrix)[high_cor[,2]],\n    correlation = cor_matrix[high_cor]\n  )\n  \n  # Sort by absolute correlation value\n  high_cor_pairs$abs_corr &lt;- abs(high_cor_pairs$correlation)\n  high_cor_pairs &lt;- high_cor_pairs[order(-high_cor_pairs$abs_corr), ]\n  high_cor_pairs$abs_corr &lt;- NULL\n  \n  cat(\"Correlation analysis complete.\\n\")\n  \n  return(list(\n    correlation_matrix = cor_matrix,\n    high_correlation_pairs = high_cor_pairs\n  ))\n}\n\n# Plot correlation matrix\nplot_correlation_matrix &lt;- function(cor_matrix) {\n  cat(\"Generating correlation plot...\\n\")\n  corrplot(cor_matrix, method = \"color\", \n           type = \"upper\", \n           tl.col = \"black\",\n           tl.srt = 45,\n           addCoef.col = \"black\",\n           number.cex = 0.7,\n           diag = FALSE,\n           col = colorRampPalette(c(\"#6D9EC1\", \"white\", \"#E46726\"))(200),\n           title = \"Correlation Matrix of Numeric Variables\")\n  cat(\"Correlation plot generated.\\n\")\n}\n\n\n\n\n5.2 VIF Analysis\n\n\nVIF analysis functions\n# Function to calculate VIF\ncalculate_vif &lt;- function(formula, data) {\n  cat(\"Calculating VIF values using full dataset...\\n\")\n  model &lt;- lm(formula, data = data)\n  vif_values &lt;- car::vif(model)\n  \n  vif_df &lt;- data.frame(\n    Variable = names(vif_values),\n    VIF = as.numeric(vif_values)\n  )\n  \n  cat(\"VIF calculation complete.\\n\")\n  return(vif_df)\n}\n\n# Plot VIF values\nplot_vif &lt;- function(vif_df, threshold = 5) {\n  cat(\"Generating VIF plot...\\n\")\n  p &lt;- ggplot(vif_df, aes(x = reorder(Variable, VIF), y = VIF)) +\n    geom_bar(stat = \"identity\", fill = ifelse(vif_df$VIF &gt; threshold, \"red\", \"steelblue\")) +\n    geom_hline(yintercept = threshold, linetype = \"dashed\", color = \"red\") +\n    geom_text(aes(label = sprintf(\"%.2f\", VIF)), hjust = -0.1) +\n    coord_flip() +\n    theme_minimal() +\n    labs(title = \"Variance Inflation Factors\",\n         subtitle = paste(\"Values above\", threshold, \"indicate potential multicollinearity\"),\n         x = \"Variables\", y = \"VIF\")\n  cat(\"VIF plot generated.\\n\")\n  return(p)\n}\n\n\n\n\n5.3 Outlier Detection\n\n\nOutlier detection functions\n# Function to check for outliers\ncheck_outliers &lt;- function(data) {\n  cat(\"Detecting outliers on full dataset...\\n\")\n  # Select only numeric variables\n  numeric_data &lt;- data %&gt;% select_if(is.numeric)\n  \n  # Use IQR method to detect outliers\n  outliers_summary &lt;- sapply(numeric_data, function(x) {\n    q1 &lt;- quantile(x, 0.25, na.rm = TRUE)\n    q3 &lt;- quantile(x, 0.75, na.rm = TRUE)\n    iqr &lt;- q3 - q1\n    lower_bound &lt;- q1 - 1.5 * iqr\n    upper_bound &lt;- q3 + 1.5 * iqr\n    sum(x &lt; lower_bound | x &gt; upper_bound, na.rm = TRUE)\n  })\n  \n  result &lt;- data.frame(\n    variable = names(outliers_summary),\n    outlier_count = outliers_summary,\n    percentage = (outliers_summary / nrow(data)) * 100\n  )\n  cat(\"Outlier detection complete.\\n\")\n  return(result)\n}\n\n# Create boxplot for a variable\ncreate_boxplot &lt;- function(data, variable) {\n  cat(\"Creating boxplot for\", variable, \"...\\n\")\n  p &lt;- ggplot(data, aes(y = .data[[variable]])) +\n    geom_boxplot(fill = \"lightblue\", outlier.color = \"red\", outlier.size = 3) +\n    theme_minimal() +\n    labs(title = paste(\"Box Plot of\", variable),\n         subtitle = \"Red points indicate potential outliers\",\n         x = \"\",\n         y = variable)\n  return(p)\n}\n\n\n\n\n5.4 Linearity Check\n\n\nLinearity check function\n# Function to check linearity assumptions with scatter plots\ncheck_linearity &lt;- function(data, dependent_var, independent_vars) {\n  plots &lt;- list()\n  \n  # Remove backticks if present for display purposes\n  display_dep_var &lt;- gsub(\"`\", \"\", dependent_var)\n  \n  for(var in independent_vars) {\n    cat(\"Checking linearity for\", display_dep_var, \"vs\", gsub(\"`\", \"\", var), \"...\\n\")\n    # Handle variable names with spaces properly\n    y_var &lt;- if(grepl(\" \", dependent_var) && !grepl(\"`\", dependent_var)) paste0(\"`\", dependent_var, \"`\") else dependent_var\n    x_var &lt;- if(grepl(\" \", var) && !grepl(\"`\", var)) paste0(\"`\", var, \"`\") else var\n    \n    p &lt;- ggplot(data, aes_string(x = x_var, y = y_var)) +\n      geom_point(alpha = 0.3) +\n      geom_smooth(method = \"loess\", color = \"red\") +\n      geom_smooth(method = \"lm\", color = \"blue\") +\n      theme_minimal() +\n      labs(title = paste(\"Relationship between\", display_dep_var, \"and\", gsub(\"`\", \"\", var)),\n           subtitle = \"Blue = linear fit, Red = loess smoothing\")\n    \n    plots[[var]] &lt;- p\n  }\n  \n  cat(\"Linearity checks complete.\\n\")\n  return(plots)\n}"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3b.html#mlr-analysis-execution",
    "href": "Prototype/Explanatory/Take-home_Ex3b.html#mlr-analysis-execution",
    "title": "MLR Model design",
    "section": "6. MLR Analysis Execution",
    "text": "6. MLR Analysis Execution\n\n6.1 Correlation Analysis\n\n\nCorrelation analysis execution\n# 1. CORRELATION ANALYSIS\ncat(\"\\n--- CORRELATION ANALYSIS ---\\n\")\nmc_result &lt;- check_multicollinearity(data)\n\n# Display correlation matrix\nplot_correlation_matrix(mc_result$correlation_matrix)\n\n# Display highly correlated variable pairs\ncat(\"\\nHighly correlated variable pairs (|r| &gt; 0.7):\\n\")\nif(nrow(mc_result$high_correlation_pairs) == 0) {\n  cat(\"No highly correlated variable pairs found.\\n\")\n} else {\n  print(mc_result$high_correlation_pairs)\n}\n\n\n\n\n\n6.2 VIF Analysis\n\n\nVIF analysis execution\n# 2. VIF ANALYSIS\ncat(\"\\n--- VIF ANALYSIS ---\\n\")\n# Define formula for initial model - use LogPrice if available, otherwise use Property Prices\ntarget_var &lt;- if(\"LogPrice\" %in% names(data)) \"LogPrice\" else \"`Property Prices`\"\ncat(\"Using\", target_var, \"as the target variable\\n\")\n\n# Define formula for initial model\ninitial_formula &lt;- as.formula(paste(target_var, \"~ Size + Floor + `Highest floor` + \n                             Units + Parking + Year + `Dist. Subway` + \n                             `Dist. CBD` + `Pop. Density` + `Higher Degree`\"))\n\n# Calculate and plot VIF\nvif_result &lt;- calculate_vif(initial_formula, data)\ncat(\"\\nVIF Results:\\n\")\nprint(vif_result)\nvif_plot &lt;- plot_vif(vif_result)\nprint(vif_plot)\n\n\n\n\nHigh VIF analysis (optional)\n# Identify variables with high VIF for potentially removing\nhigh_vif_vars &lt;- vif_result$Variable[vif_result$VIF &gt; 5]\nif(length(high_vif_vars) &gt; 0) {\n  cat(\"\\nVariables with high VIF (&gt;5):\", paste(high_vif_vars, collapse=\", \"), \"\\n\")\n  cat(\"Consider removing these variables to reduce multicollinearity.\\n\")\n  \n  # Example of creating a reduced model formula without the highest VIF variable\n  if(length(high_vif_vars) &gt; 0) {\n    highest_vif_var &lt;- vif_result$Variable[which.max(vif_result$VIF)]\n    cat(\"\\nCreating a reduced model by removing\", highest_vif_var, \"...\\n\")\n    \n    reduced_formula &lt;- as.formula(paste(target_var, \"~\", \n                                      paste(setdiff(vif_result$Variable, highest_vif_var), \n                                            collapse = \" + \")))\n    \n    reduced_vif &lt;- calculate_vif(reduced_formula, data)\n    cat(\"\\nVIF Results after removing\", highest_vif_var, \":\\n\")\n    print(reduced_vif)\n    reduced_vif_plot &lt;- plot_vif(reduced_vif)\n    print(reduced_vif_plot)\n  }\n}\n\n\n\n\n\n\n\n6.3 Outlier Detection\n\n\nOutlier detection execution\n# 3. OUTLIER DETECTION\ncat(\"\\n--- OUTLIER DETECTION ---\\n\")\noutliers &lt;- check_outliers(data)\ncat(\"\\nOutlier Detection Summary (sorted by count):\\n\")\nprint(outliers[order(-outliers$outlier_count), ])\n\n# Create boxplots for key variables using sample data\ncat(\"\\nGenerating boxplots for key variables...\\n\")\nkey_vars &lt;- c(\"Property Prices\", \"Size\", \"Floor\", \"Units\", \"Dist. Subway\")\nif(\"LogPrice\" %in% names(data)) {\n  key_vars &lt;- c(key_vars, \"LogPrice\")\n}\n\nboxplots &lt;- list()\nfor(var in key_vars) {\n  boxplots[[var]] &lt;- create_boxplot(viz_data, var)\n  print(boxplots[[var]])\n}\n\n\n\n\n\n\n\n6.4 Linearity Checks\n\n\nLinearity checks - original variables\n# 4. LINEARITY CHECKS\ncat(\"\\n--- LINEARITY CHECKS ---\\n\")\n# Check linearity for key variables using sample data\nindep_vars &lt;- c(\"Size\", \"Floor\", \"Year\", \"Dist. Subway\", \"Dist. CBD\")\n\ncat(\"\\nChecking linearity with original Property Prices...\\n\")\nlinearity_original &lt;- check_linearity(viz_data, \"Property Prices\", indep_vars)\nfor(i in seq_along(linearity_original)) {\n  print(linearity_original[[i]])\n}\n\n\n\n\nLinearity checks - log-transformed variables\n# Linearity checks with log-transformed target\nif(\"LogPrice\" %in% names(data)) {\n  cat(\"\\nChecking linearity with log-transformed Property Prices (LogPrice)...\\n\")\n  linearity_log &lt;- check_linearity(viz_data, \"LogPrice\", indep_vars)\n  for(i in seq_along(linearity_log)) {\n    print(linearity_log[[i]])\n  }\n  \n  # Compare linear fit of original vs transformed\n  cat(\"\\nComparing linearity improvement with log transformation...\\n\")\n  for(i in seq_along(indep_vars)) {\n    var &lt;- indep_vars[i]\n    var_display &lt;- gsub(\"`\", \"\", var)\n    p1 &lt;- linearity_original[[i]] + ggtitle(paste(\"Original:\", var_display, \"vs Property Prices\"))\n    p2 &lt;- linearity_log[[i]] + ggtitle(paste(\"Log-transformed:\", var_display, \"vs LogPrice\"))\n    comparison &lt;- gridExtra::grid.arrange(p1, p2, ncol = 2)\n    print(comparison)\n  }\n}\n\n\n\n\nScatterplot matrix\n# 5. SCATTERPLOT MATRIX (using sample data)\ncat(\"\\n--- SCATTERPLOT MATRIX ---\\n\")\ncat(\"Creating scatterplot matrix (this may take a while)...\\n\")\nkey_vars_for_pairs &lt;- c(\"Property Prices\", \"Size\", \"Floor\", \"Dist. Subway\", \"Pop. Density\")\nif(\"LogPrice\" %in% names(viz_data)) {\n  key_vars_for_pairs[1] &lt;- \"LogPrice\"  # Replace Property Prices with LogPrice if available\n}\npairs_plot &lt;- GGally::ggpairs(viz_data[, key_vars_for_pairs])\nprint(pairs_plot)\n\ncat(\"\\nMLR pre-check analysis complete!\\n\")"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3b.html#final-model-fitting",
    "href": "Prototype/Explanatory/Take-home_Ex3b.html#final-model-fitting",
    "title": "MLR Model design",
    "section": "7. Final Model Fitting",
    "text": "7. Final Model Fitting\nBased on the pre-check analysis, a final model is developed with carefully selected variables.\n\n\nFinal model fitting\n# 7. RECOMMENDED FINAL MODEL\ncat(\"\\n--- RECOMMENDED FINAL MODEL ---\\n\")\n# Based on analysis, suggest a final model\nif(\"LogPrice\" %in% names(data)) {\n  # Create a final model including significant predictors\n  model_formula &lt;- paste(\"LogPrice ~ Size + Floor + Units + Parking + Year +\",\n                        \"`Dist. Subway` + `Dist. CBD` + `Pop. Density` +\",\n                        \"`Higher Degree` + Spring + Fall + Winter\")\n  \n  cat(\"Recommended model formula based on analysis:\\n\")\n  cat(model_formula, \"\\n\\n\")\n  \n  # Fit the final model\n  cat(\"Fitting final model...\\n\")\n  final_model &lt;- lm(as.formula(model_formula), data = data)\n  \n  # Display model summary\n  cat(\"\\nFinal Model Summary:\\n\")\n  print(summary(final_model))\n  \n  # Calculate adjusted R-squared\n  cat(\"\\nAdjusted R-squared:\", summary(final_model)$adj.r.squared, \"\\n\")\n  \n  # Display ANOVA table\n  cat(\"\\nANOVA Table:\\n\")\n  print(anova(final_model))\n} else {\n  cat(\"No log-transformed target variable (LogPrice) found. Consider creating one before fitting the final model.\\n\")\n}"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3b.html#model-diagnostics",
    "href": "Prototype/Explanatory/Take-home_Ex3b.html#model-diagnostics",
    "title": "MLR Model design",
    "section": "8. Model Diagnostics",
    "text": "8. Model Diagnostics\nDiagnostic checks ensure that the model meets regression assumptions.\n\n\nResidual plots\n# Check residuals\ncat(\"\\n--- FINAL MODEL DIAGNOSTICS ---\\n\")\n\n# Create residual plots\ncat(\"Generating residual plots...\\n\")\n\n# Setting a larger plotting area with smaller margins before creating plots\npar(mfrow = c(2, 2), mar = c(4, 4, 2, 1))  # Reduce margins\nplot(final_model)\n\n# Reset plotting parameters\npar(mfrow = c(1, 1), mar = c(5, 4, 4, 2) + 0.1)  # Reset to default margins\n\n\n\n\nStatistical diagnostic tests\n# Shapiro-Wilk test on a sample of residuals (since dataset is large)\nresiduals_sample &lt;- sample(residuals(final_model), min(5000, length(residuals(final_model))))\nsw_test &lt;- shapiro.test(residuals_sample)\ncat(\"\\nShapiro-Wilk normality test on residuals (sample):\\n\")\nprint(sw_test)\n\n# Check for heteroscedasticity with Breusch-Pagan test\ncat(\"\\nBreusch-Pagan test for heteroscedasticity:\\n\")\nbp_test &lt;- car::ncvTest(final_model)\nprint(bp_test)"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3b.html#variable-importance-analysis",
    "href": "Prototype/Explanatory/Take-home_Ex3b.html#variable-importance-analysis",
    "title": "MLR Model design",
    "section": "9. Variable Importance Analysis",
    "text": "9. Variable Importance Analysis\n\n\nVariable importance analysis\n# Load the lm.beta package for standardized coefficients\nif(!require(lm.beta)) install.packages(\"lm.beta\")\nlibrary(lm.beta)\n\n# Variable importance\ncat(\"\\nVariable Importance (standardized coefficients):\\n\")\nstd_coef &lt;- lm.beta::lm.beta(final_model)\nprint(std_coef)\n\n# Plot variable importance\ncoef_df &lt;- data.frame(\n  Variable = names(std_coef$standardized.coefficients),\n  Importance = abs(std_coef$standardized.coefficients)\n)\ncoef_df &lt;- coef_df[-1, ]  # Remove intercept\n\nimportance_plot &lt;- ggplot(coef_df, aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(title = \"Variable Importance in Final Model\",\n       subtitle = \"Based on standardized coefficients\",\n       x = \"Variables\",\n       y = \"Absolute Standardized Coefficient\")\n\nprint(importance_plot)"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3b.html#model-prediction-accuracy",
    "href": "Prototype/Explanatory/Take-home_Ex3b.html#model-prediction-accuracy",
    "title": "MLR Model design",
    "section": "10. Model Prediction Accuracy",
    "text": "10. Model Prediction Accuracy\n\n\nPrediction accuracy assessment\n# Prediction accuracy assessment using 10% of data\ncat(\"\\nAssessing prediction accuracy on random sample...\\n\")\nset.seed(123)  # For reproducibility\nsample_indices &lt;- sample(nrow(data), nrow(data) * 0.1)\ntest_data &lt;- data[sample_indices, ]\npredicted_values &lt;- predict(final_model, newdata = test_data)\n\n# Calculate RMSE and MAE\nrmse &lt;- sqrt(mean((test_data$LogPrice - predicted_values)^2))\nmae &lt;- mean(abs(test_data$LogPrice - predicted_values))\n\ncat(\"Root Mean Square Error (RMSE):\", rmse, \"\\n\")\ncat(\"Mean Absolute Error (MAE):\", mae, \"\\n\")\n\n# Convert back to original scale for interpretation\nrmse_original &lt;- mean((exp(test_data$LogPrice) - exp(predicted_values))^2)\nmae_original &lt;- mean(abs(exp(test_data$LogPrice) - exp(predicted_values)))\n\ncat(\"Mean Squared Error on original scale:\", rmse_original, \"\\n\")\ncat(\"Mean Absolute Error on original scale:\", mae_original, \"\\n\")\n\n\n\n\nPrediction plot\n# Prediction plot\nprediction_df &lt;- data.frame(\n  Actual = test_data$LogPrice,\n  Predicted = predicted_values\n)\n\nprediction_plot &lt;- ggplot(prediction_df, aes(x = Actual, y = Predicted)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Actual vs. Predicted Values\",\n       subtitle = \"Red line indicates perfect prediction\",\n       x = \"Actual LogPrice\",\n       y = \"Predicted LogPrice\")\n\nprint(prediction_plot)"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3b.html#final-model-equation-and-conclusion",
    "href": "Prototype/Explanatory/Take-home_Ex3b.html#final-model-equation-and-conclusion",
    "title": "MLR Model design",
    "section": "11. Final Model Equation and Conclusion",
    "text": "11. Final Model Equation and Conclusion\n\n\nFinal model equation\n# Final model equation\ncat(\"\\nFinal MLR Model Equation:\\n\")\ncoef_values &lt;- coef(final_model)\nequation &lt;- paste(\"LogPrice =\", round(coef_values[1], 4))\n\nfor (i in 2:length(coef_values)) {\n  var_name &lt;- names(coef_values)[i]\n  coef_value &lt;- coef_values[i]\n  \n  if (coef_value &gt;= 0) {\n    equation &lt;- paste(equation, \"+\", round(coef_value, 4), \"*\", var_name)\n  } else {\n    equation &lt;- paste(equation, round(coef_value, 4), \"*\", var_name)\n  }\n}\n\ncat(equation, \"\\n\")\ncat(\"\\nMLR analysis complete!\\n\")"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3b.html#shiny-dashboard-ui-design",
    "href": "Prototype/Explanatory/Take-home_Ex3b.html#shiny-dashboard-ui-design",
    "title": "MLR Model design",
    "section": "12. Shiny Dashboard UI Design",
    "text": "12. Shiny Dashboard UI Design\nBased on the provided screenshots, the Shiny Dashboard application has been designed with the following components:\n\n12.1 Overall Layout\n\nHeader with ‚ÄúProperty Price‚Äù title and menu toggle\nGreen themed sidebar for MLR model building\nMain panel with tabbed interface for different analysis aspects\n\n\n\n12.2 Sidebar Components\n\nMLR Model Builder section\nVariable selection panels organized by categories:\n\nProperty Characteristics (Size, Floor, Units, Parking, Year)\nLocation Variables (Dist. Subway, Dist. CBD, Pop. Density)\nDemographic Variables (Higher Degree)\nSeasonal Variables (Spring, Fall, Winter)\n\nModel Settings section with:\n\nVIF Threshold slider (default set to 5)\nTarget Variable Transformation radio buttons (Original, Log Transform, Square Root)\n\nAction buttons:\n\n‚ÄúRun MLR Analysis‚Äù button\n‚ÄúUse Recommended Model‚Äù button\n\n\n\n\n12.3 Main Panel Tabs\n\nModel Summary: Displays regression equation, coefficients table, and model fit statistics\nVariable Importance: Shows standardized coefficients in a bar chart format\nMulticollinearity: Presents correlation matrix and VIF analysis\nModel Diagnostics: Shows residual plots and diagnostic test results\n\n\n\n12.4 Visualization Components\n\nModel Diagnostics Tab:\n\nResiduals vs Fitted plot showing model fit\nNormal Q-Q plot for assessing normality\nScale-Location plot for checking homoscedasticity\nResiduals histogram\nDiagnostic test results for normality and heteroscedasticity\n\n\nMulticollinearity Tab:\n\nCorrelation matrix heatmap with coefficient values\nVariance Inflation Factors bar chart\nTable of highly correlated variable pairs\n\n\nVariable Importance Tab:\n\nBar chart of standardized coefficients\nInterpretation panel explaining relative variable importance\nKey findings summary\n\n\nModel Summary Tab:\n\nScrollable regression equation\nDetailed coefficient table with estimates, standard errors, t-values and p-values\nModel fit statistics panel showing R-squared, Adjusted R-squared, F-statistic, and Residual Standard Error\n\n\n\n\n\nShiny Dashboard UI\n# UI design for the Shiny dashboard\nui &lt;- dashboardPage(\n  dashboardHeader(title = \"Property Price\"),\n  \n  dashboardSidebar(\n    width = 300,\n    tags$div(style = \"background-color: #9DC183; padding: 10px; margin-top: -15px; margin-bottom: 15px;\",\n             h4(\"Multiple Linear Regression\", style = \"color: #FFF;\")),\n    \n    # Model Builder Box\n    box(\n      title = \"MLR Model Builder\", \n      width = NULL, \n      solidHeader = TRUE, \n      status = \"success\",\n      collapsible = FALSE,\n      \n      # Variable selection sections\n      h4(\"Select Variables:\"),\n      \n      # Property Characteristics\n      h5(\"Property Characteristics:\"),\n      div(\n        style = \"border: 1px solid #ddd; padding: 10px; margin-bottom: 10px; border-radius: 5px; background-color: #f9f9f9;\",\n        fluidRow(\n          column(width = 12,\n                 div(\n                   style = \"margin-bottom: 5px;\",\n                   tags$button(\"Size\", class = \"btn btn-sm\", style = \"background-color: #D4EAC7; margin: 2px;\"),\n                   tags$button(\"Floor\", class = \"btn btn-sm\", style = \"background-color: #D4EAC7; margin: 2px;\"),\n                   tags$button(\"Units\", class = \"btn btn-sm\", style = \"background-color: #D4EAC7; margin: 2px;\"),\n                   tags$button(\"Parking\", class = \"btn btn-sm\", style = \"background-color: #D4EAC7; margin: 2px;\"),\n                   tags$button(\"Year\", class = \"btn btn-sm\", style = \"background-color: #D4EAC7; margin: 2px;\")\n                 )\n          )\n        ),\n        tags$div(\"Select Variables\", style = \"color: #666; font-size: 12px;\")\n      ),\n      \n      # Location Variables\n      h5(\"Location Variables:\"),\n      div(\n        style = \"border: 1px solid #ddd; padding: 10px; margin-bottom: 10px; border-radius: 5px; background-color: #f9f9f9;\",\n        fluidRow(\n          column(width = 12,\n                 div(\n                   style = \"margin-bottom: 5px;\",\n                   tags$button(\"Dist. Subway\", class = \"btn btn-sm\", style = \"background-color: #D4EAC7; margin: 2px;\"),\n                   tags$button(\"Dist. CBD\", class = \"btn btn-sm\", style = \"background-color: #D4EAC7; margin: 2px;\"),\n                   tags$button(\"Pop. Density\", class = \"btn btn-sm\", style = \"background-color: #D4EAC7; margin: 2px;\")\n                 )\n          )\n        ),\n        tags$div(\"Select Variables\", style = \"color: #666; font-size: 12px;\")\n      ),\n      \n      # Demographic Variables\n      h5(\"Demographic Variables:\"),\n      div(\n        style = \"border: 1px solid #ddd; padding: 10px; margin-bottom: 10px; border-radius: 5px; background-color: #f9f9f9;\",\n        fluidRow(\n          column(width = 12,\n                 div(\n                   style = \"margin-bottom: 5px;\",\n                   tags$button(\"Higher Degree\", class = \"btn btn-sm\", style = \"background-color: #D4EAC7; margin: 2px;\")\n                 )\n          )\n        ),\n        tags$div(\"Select Variables\", style = \"color: #666; font-size: 12px;\")\n      ),\n      \n      # Seasonal Variables\n      h5(\"Seasonal Variables:\"),\n      div(\n        style = \"border: 1px solid #ddd; padding: 10px; margin-bottom: 10px; border-radius: 5px; background-color: #f9f9f9;\",\n        fluidRow(\n          column(width = 12,\n                 div(\n                   style = \"margin-bottom: 5px;\",\n                   tags$button(\"Spring\", class = \"btn btn-sm\", style = \"background-color: #D4EAC7; margin: 2px;\"),\n                   tags$button(\"Fall\", class = \"btn btn-sm\", style = \"background-color: #D4EAC7; margin: 2px;\"),\n                   tags$button(\"Winter\", class = \"btn btn-sm\", style = \"background-color: #D4EAC7; margin: 2px;\")\n                 )\n          )\n        ),\n        tags$div(\"Select Variables\", style = \"color: #666; font-size: 12px;\")\n      ),\n      \n      # Model Settings\n      h4(\"Model Settings:\"),\n      \n      # VIF Threshold\n      h5(\"VIF Threshold:\"),\n      sliderInput(\"vif_threshold\", NULL, min = 1, max = 10, value = 5, step = 0.5),\n      tags$div(\"Variables with VIF above threshold will be highlighted\", style = \"color: #666; font-size: 12px;\"),\n      \n      # Target Variable Transformation\n      h5(\"Target Variable Transformation:\"),\n      radioButtons(\"transformation\", NULL, \n                  choices = list(\"Original\" = \"original\", \n                                \"Log Transform\" = \"log\", \n                                \"Square Root\" = \"sqrt\"),\n                  selected = \"log\"),\n      \n      # Action Buttons\n      actionButton(\"run_analysis\", \"Run MLR Analysis\", \n                  style = \"background-color: #29793B; color: white; width: 100%;\"),\n      br(), br(),\n      actionButton(\"use_recommended\", \"Use Recommended Model\", \n                  style = \"background-color: #9DC183; color: white; width: 100%;\")\n    )\n  ),\n  \n  dashboardBody(\n    tags$head(\n      tags$style(HTML(\"\n        .content-wrapper, .right-side {\n          background-color: #f5f5f5;\n        }\n        .box {\n          border-top: 3px solid #9DC183;\n        }\n        .nav-tabs-custom&gt;.nav-tabs&gt;li.active {\n          border-top-color: #9DC183;\n        }\n      \"))\n    ),\n    \n    fluidRow(\n      column(width = 12,\n             div(style = \"background-color: #9DC183; padding: 5px; margin-bottom: 15px;\",\n                 h4(\"Multiple Linear Regression\", style = \"color: #FFF;\"))\n      )\n    ),\n    \n    tabBox(\n      width = 12,\n      id = \"tabbox\",\n      tabPanel(\"Model Summary\", value = \"summary\",\n               fluidRow(\n                 box(\n                   title = \"Regression Equation\", width = 12, status = \"success\",\n                   div(\n                     style = \"overflow-x: auto; white-space: nowrap;\",\n                     \"LogPrice = -0.4073 + 0.0012 * Size + 5e-04 * Floor + 0 * Units + 0.01 * Parking + 0.0013 * Year -0.0065 * `Dist. Subway` + 0 * `Dist. CBD` + 0 * `Pop. Density` + 7e-04 * `Higher Degree` + 0.0014 * Spring1 + 0.0076 * Fall1 + 0.0048 * Winter1\"\n                   )\n                 )\n               ),\n               fluidRow(\n                 box(\n                   title = \"Model Coefficients\", width = 8, status = \"success\",\n                   div(style = \"overflow-x: auto;\",\n                       dataTableOutput(\"coefficients_table\")\n                   )\n                 ),\n                 column(\n                   width = 4,\n                   box(\n                     title = \"R-squared\", width = NULL, status = \"success\",\n                     valueBox(\"0.6675\", \"R-squared\", icon = NULL, color = \"green\", width = 12)\n                   ),\n                   box(\n                     title = \"Adjusted R-squared\", width = NULL, status = \"success\",\n                     valueBox(\"0.6674\", \"Adjusted R-squared\", icon = NULL, color = \"green\", width = 12)\n                   ),\n                   box(\n                     title = \"F-statistic\", width = NULL, status = \"success\",\n                     valueBox(\"8803.27 on 12 and 52631 DF, p &lt; 0.001\", \"F-statistic\", icon = NULL, color = \"green\", width = 12)\n                   ),\n                   box(\n                     title = \"Residual Standard Error\", width = NULL, status = \"success\",\n                     valueBox(\"0.0323\", \"Residual Std. Error\", icon = NULL, color = \"green\", width = 12)\n                   )\n                 )\n               )\n      ),\n      tabPanel(\"Variable Importance\", value = \"importance\",\n               fluidRow(\n                 box(\n                   title = \"Variable Importance\", width = 12, status = \"success\",\n                   plotOutput(\"variable_importance_plot\", height = \"500px\")\n                 )\n               ),\n               fluidRow(\n                 box(\n                   title = \"Interpretation\", width = 12, status = \"success\",\n                   h4(\"Key Findings:\"),\n                   p(\"The standardized coefficients represent the relative importance of each predictor variable in the model.\"),\n                   \n                   h5(\"Most Influential Positive Factors:\"),\n                   tags$ul(\n                     tags$li(strong(\"Size: 0.6163\"), \"(38.4% influence)\"),\n                     tags$li(strong(\"Year: 0.2374\"), \"(14.8% influence)\"),\n                     tags$li(strong(\"Higher Degree: 0.1298\"), \"(8.1% influence)\")\n                   ),\n                   \n                   h5(\"Most Influential Negative Factors:\"),\n                   tags$ul(\n                     tags$li(strong(\"Dist. Subway: -0.1185\"), \"(7.4% influence)\")\n                   ),\n                   \n                   tags$p(style = \"color: #666; font-style: italic;\", \n                          \"Note: The standardized coefficients allow direct comparison between variables measured on different scales. \n                          Variables with higher absolute values have a stronger effect on the property price.\")\n                 )\n               )\n      ),\n      tabPanel(\"Multicollinearity\", value = \"multicollinearity\",\n               fluidRow(\n                 box(\n                   title = \"Correlation Matrix\", width = 6, status = \"success\",\n                   plotOutput(\"correlation_matrix\", height = \"400px\")\n                 ),\n                 box(\n                   title = \"Variance Inflation Factors\", width = 6, status = \"success\",\n                   plotOutput(\"vif_plot\", height = \"400px\")\n                 )\n               ),\n               fluidRow(\n                 box(\n                   title = \"Highly Correlated Variables\", width = 12, status = \"success\",\n                   div(\n                     dataTableOutput(\"high_correlation_table\"),\n                     p(\"Showing 1 to 1 of 1 entries\")\n                   )\n                 )\n               )\n      ),\n      tabPanel(\"Model Diagnostics\", value = \"diagnostics\",\n               fluidRow(\n                 box(\n                   title = \"Residuals vs Fitted\", width = 6, status = \"success\",\n                   plotOutput(\"residuals_fitted_plot\", height = \"300px\")\n                 ),\n                 box(\n                   title = \"Normal Q-Q Plot\", width = 6, status = \"success\",\n                   plotOutput(\"qq_plot\", height = \"300px\")\n                 )\n               ),\n               fluidRow(\n                 box(\n                   title = \"Scale-Location Plot\", width = 6, status = \"success\",\n                   plotOutput(\"scale_location_plot\", height = \"300px\")\n                 ),\n                 box(\n                   title = \"Residuals Histogram\", width = 6, status = \"success\",\n                   plotOutput(\"residuals_histogram\", height = \"300px\")\n                 )\n               ),\n               fluidRow(\n                 valueBox(\n                   value = \"W = 0.991, p &lt; 0.001 (Non-normal)\",\n                   subtitle = \"Shapiro-Wilk Test (Normality)\",\n                   icon = icon(\"check-circle\"),\n                   color = \"green\",\n                   width = 6\n                 ),\n                 valueBox(\n                   value = \"œá¬≤ = 9.7, p = 0.002 (Heteroscedastic)\",\n                   subtitle = \"Breusch-Pagan Test (Heteroscedasticity)\",\n                   icon = icon(\"exclamation-circle\"),\n                   color = \"green\",\n                   width = 6\n                 )\n               )\n      )\n    )\n  )\n)\n\n\n\n\nShiny Dashboard Server\n# Server logic for the Shiny dashboard\nserver &lt;- function(input, output, session) {\n  \n  # Render coefficients table\n  output$coefficients_table &lt;- renderDataTable({\n    data.frame(\n      Variable = c(\"(Intercept)\", \"Size\", \"Floor\", \"Units\", \"Parking\", \"Year\", \n                  \"Dist. Subway\", \"Dist. CBD\", \"Pop. Density\", \"Higher Degree\", \n                  \"Spring1\", \"Fall1\", \"Winter1\"),\n      Estimate = c(-0.4073, 0.0012, 0.0005, 0.0000, 0.0100, 0.0013, -0.0065, \n                   0.0000, 0.0000, 0.0007, 0.0014, 0.0076, 0.0048),\n      `Std. Error` = c(0.0292, 0.0000, 0.0000, 0.0000, 0.0003, 0.0000, 0.0001, \n                       0.0000, 0.0000, 0.0000, 0.0004, 0.0004, 0.0004),\n      `t value` = c(-13.9269, 220.9179, 26.7469, 44.6102, 39.6990, 89.5777, \n                    -45.8230, 11.1325, 23.7707, 90.1631, 3.2289, 18.9962, 11.3244),\n      `p-value` = c(\"5.21e-44\", \"0.00\", \"1.50e-156\", \"0.00\", \"0.00\", \"0.00\", \n                    \"0.00\", \"9.38e-29\", \"3.04e-124\", \"0.00\", \"0.00124\", \"4.11e-80\", \"1.08e-29\"),\n      Significance = c(\"***\", \"***\", \"***\", \"***\", \"***\", \"***\", \n                       \"***\", \"***\", \"***\", \"***\", \"**\", \"***\", \"***\")\n    ),\n    options = list(\n      pageLength = 13,\n      searching = TRUE,\n      lengthChange = FALSE,\n      dom = 'lrtip'\n    )\n  })\n  \n  # Render variable importance plot\n  output$variable_importance_plot &lt;- renderPlot({\n    # Create sample data for the plot\n    importance_data &lt;- data.frame(\n      Variable = c(\"Size\", \"Year\", \"Higher Degree\", \"Dist. Subway\", \"Units\", \n                   \"Parking\", \"Floor\", \"Fall1\", \"Pop. Density\", \"Winter1\", \n                   \"Dist. CBD\", \"Spring1\"),\n      Importance = c(0.6163, 0.2374, 0.1298, -0.1185, 0.1184, 0.1103, 0.0723, \n                     0.0645, 0.0602, 0.0370, 0.0286, 0.0104)\n    )\n    \n    # Create the plot\n    ggplot(importance_data, aes(x = reorder(Variable, Importance), y = Importance)) +\n      geom_bar(stat = \"identity\", fill = \"#9DC183\") +\n      coord_flip() +\n      theme_minimal() +\n      labs(title = \"Variable Importance (Standardized Coefficients)\",\n           x = \"Variable\",\n           y = \"Standardized Coefficient\") +\n      theme(\n        plot.title = element_text(size = 16, face = \"bold\"),\n        axis.title = element_text(size = 12),\n        axis.text = element_text(size = 10)\n      )\n  })\n  \n  # Render correlation matrix\n  output$correlation_matrix &lt;- renderPlot({\n    # This is a placeholder - in a real app, you would calculate this from the data\n    sample_cor_matrix &lt;- matrix(c(\n      1.00, 0.48, 0.68, 0.14, 0.53, 0.00, -0.51, 0.00, -0.17, 0.11,\n      0.48, 1.00, 0.24, 0.08, 0.08, -0.03, 0.00, 0.00, -0.12, 0.09,\n      0.68, 0.24, 1.00, 0.13, 0.00, 0.00, -0.12, 0.00, -0.03, 0.00,\n      0.14, 0.08, 0.13, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.10,\n      0.53, 0.08, 0.00, 0.00, 1.00, 0.03, -0.03, 0.00, -0.09, 0.00,\n      0.00, -0.03, 0.00, 0.00, 0.03, 1.00, -0.17, 0.00, -0.11, -0.08,\n      -0.51, 0.00, -0.12, 0.00, -0.03, -0.17, 1.00, 0.00, 0.13, -0.12,\n      0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00,\n      -0.17, -0.12, -0.03, 0.00, -0.09, -0.11, 0.13, 0.00, 1.00, 0.04,\n      0.11, 0.09, 0.00, 0.10, 0.00, -0.08, -0.12, 0.00, 0.04, 1.00\n    ), nrow = 10)\n    \n    colnames(sample_cor_matrix) &lt;- rownames(sample_cor_matrix) &lt;- c(\"Size\", \"Floor\", \"Units\", \n                                                                   \"Parking\", \"Year\", \"Dist. Subway\", \n                                                                   \"Dist. CBD\", \"Pop. Density\", \n                                                                   \"Higher Degree\", \"LogPrice\")\n    \n    # Plot correlation matrix\n    corrplot(sample_cor_matrix, method = \"color\", \n             type = \"upper\", \n             tl.col = \"black\", \n             tl.srt = 45, \n             addCoef.col = \"black\", \n             number.cex = 0.7,\n             diag = FALSE,\n             title = \"Correlation Matrix of Numeric Variables\")\n  })\n  \n  # Render VIF plot\n  output$vif_plot &lt;- renderPlot({\n    # Sample VIF data\n    vif_data &lt;- data.frame(\n      Variable = c(\"Year\", \"Winter\", \"Units\", \"Spring\", \"Size\", \"Parking\", \n                  \"Floor\", \"Fall\", \"Pop. Density\", \"Higher Degree\", \"Dist. Subway\", \n                  \"Dist. CBD\"),\n      VIF = c(1.17, 1.05, 1.31, 1.02, 1.22, 1.12, 1.31, 1.07, 1.08, 1.17, 1.12, 1.05)\n    )\n    \n    # Create VIF plot\n    ggplot(vif_data, aes(x = reorder(Variable, VIF), y = VIF)) +\n      geom_bar(stat = \"identity\", fill = \"#9DC183\") +\n      geom_hline(yintercept = 5, linetype = \"dashed\", color = \"red\") +\n      coord_flip() +\n      theme_minimal() +\n      labs(title = \"Variance Inflation Factors\",\n           x = \"Variable\",\n           y = \"VIF\") +\n      theme(\n        plot.title = element_text(size = 16, face = \"bold\"),\n        axis.title = element_text(size = 12),\n        axis.text = element_text(size = 10)\n      )\n  })\n  \n  # Render high correlation table\n  output$high_correlation_table &lt;- renderDataTable({\n    data.frame(\n      Message = c(\"No high correlations detected (r &gt; 0.7)\")\n    ),\n    options = list(\n      dom = 't',\n      paging = FALSE,\n      searching = FALSE\n    )\n  })\n  \n  # Model diagnostic plots\n  output$residuals_fitted_plot &lt;- renderPlot({\n    # This would normally be generated from the model\n    # For demonstration purposes, using a static image representation\n    ggplot() + \n      annotate(\"text\", x = 0.5, y = 0.5, label = \"Residuals vs Fitted Plot would appear here\", size = 5) +\n      theme_void()\n  })\n  \n  output$qq_plot &lt;- renderPlot({\n    # Static representation\n    ggplot() + \n      annotate(\"text\", x = 0.5, y = 0.5, label = \"Normal Q-Q Plot would appear here\", size = 5) +\n      theme_void()\n  })\n  \n  output$scale_location_plot &lt;- renderPlot({\n    # Static representation\n    ggplot() + \n      annotate(\"text\", x = 0.5, y = 0.5, label = \"Scale-Location Plot would appear here\", size = 5) +\n      theme_void()\n  })\n  \n  output$residuals_histogram &lt;- renderPlot({\n    # Static representation\n    ggplot() + \n      annotate(\"text\", x = 0.5, y = 0.5, label = \"Residuals Histogram would appear here\", size = 5) +\n      theme_void()\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3b.html#conclusion",
    "href": "Prototype/Explanatory/Take-home_Ex3b.html#conclusion",
    "title": "MLR Model design",
    "section": "13. Conclusion",
    "text": "13. Conclusion\n\nKey Findings\n\nSize is the most influential predictor, with a standardized coefficient of 0.6163\nYear is the second most important predictor (0.2374), indicating newer properties command higher prices\nHigher Degree (0.1298) represents neighborhood socioeconomic status and has a significant positive impact\nDist. Subway (-0.1185) has a negative impact, confirming properties closer to subway stations are more valuable\nThe model has strong predictive power, explaining 66.75% of the variation in log-transformed property prices (R¬≤ = 0.6675)\n\n\n\nModel Performance\n\nAll variables in the final model are statistically significant (p &lt; 0.01)\nNo severe multicollinearity detected (all VIF values &lt; 5)\nThe model‚Äôs prediction accuracy is good, with RMSE = 0.0316 in log-price units\n\n\n\nLimitations and Future Work\n\nMinor heteroscedasticity detected (Breusch-Pagan test: œá¬≤ = 9.7, p = 0.002)\nSlight deviation from normality in residuals (Shapiro-Wilk test: W = 0.991, p &lt; 0.001)\nFuture work could explore non-linear relationships and interaction effects\n\nThe MLR model provides valuable insights into property price determinants, with practical applications for real estate valuation, investment decision-making, and urban planning."
  },
  {
    "objectID": "Prototype/EDA/Take-home_Ex03.html",
    "href": "Prototype/EDA/Take-home_Ex03.html",
    "title": "Exploratory Data Analysis & Confirmatory Data Analysis",
    "section": "",
    "text": "1 Background\nIn this take-home exercise, I will be prototyping a shiny module for my group project, which aims to investigate how various factors, with a greater focus on the environmental amenities and the built environment factors, influences property prices in Korea.\nTHe objectives of this exercise includes:\n\nTo evaluate and determine the necessary R packages needed for the Shiny application are supported in R CRAN,\nTo prepare and test the specific R codes can be run and return the correct output as expected,\nTo determine the parameters and outputs that should be exposed on the Shiny applications, and\nTo select the appropriate Shiny UI components for exposing the parameters determine above.\n\nSpecifically, I will be working on a shiny module to perform Exploratory Data ANalysis (EDA) and Confirmatory Data Analysis (CDA).\n\n\n2 Getting Started\n\nLoading LibrariesImporting DatasetGlimpse dataMissing dataDuplicated Records\n\n\nWe will load the following packages:\n\ntidyverse to wrangle data\nSmartEDA to explore data\nggdist for visualising distributions and uncertainty\nparallelPlot for plotting parallel coordinates plot with histogram\nggstatsplot for correlation heat map\nggside for adding histograms to scatterplot\nplotly for interactive plots\nggpubr to add statistical test results\n\n\npacman::p_load(tidyverse,  SmartEDA, ggdist, parallelPlot,  ggstatsplot, ggside, plotly, ggpubr)\n\n\n\nIn this exercise, we will be using the exam data\n\nProperty_data &lt;- read_csv(\"data/Property_Price_and_Green_Index.csv\")\n\n\n\nMajority of the variables in the dataset are continuous variables\n\nglimpse(Property_data)\n\nRows: 52,644\nColumns: 28\n$ `Property Prices`  &lt;dbl&gt; 9.798127, 9.852194, 9.740969, 9.798127, 9.692767, 9‚Ä¶\n$ Longitude          &lt;dbl&gt; 129.1081, 129.1081, 129.1081, 129.1081, 129.1081, 1‚Ä¶\n$ Latitude           &lt;dbl&gt; 35.21502, 35.21502, 35.21502, 35.21502, 35.21502, 3‚Ä¶\n$ Size               &lt;dbl&gt; 45.0700, 38.1000, 45.0700, 38.1000, 38.1000, 45.070‚Ä¶\n$ Floor              &lt;dbl&gt; 8, 13, 6, 13, 7, 9, 6, 6, 11, 7, 9, 9, 10, 11, 2, 4‚Ä¶\n$ `Highest floor`    &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 11, 11,‚Ä¶\n$ Units              &lt;dbl&gt; 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 8, 8, 8‚Ä¶\n$ Parking            &lt;dbl&gt; 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.6‚Ä¶\n$ Heating            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ Year               &lt;dbl&gt; 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 201‚Ä¶\n$ `Dist. Green`      &lt;dbl&gt; 4.668050, 4.668050, 4.668050, 4.668050, 4.668050, 4‚Ä¶\n$ `Dist. Water`      &lt;dbl&gt; 7.092015, 7.092015, 7.092015, 7.092015, 7.092015, 7‚Ä¶\n$ `Green Index`      &lt;dbl&gt; 10.867812, 10.867812, 10.867812, 10.867812, 10.8678‚Ä¶\n$ `Dist. Subway`     &lt;dbl&gt; 5.655021, 5.655021, 5.655021, 5.655021, 5.655021, 5‚Ä¶\n$ `Bus Stop`         &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 13, 13, 13, ‚Ä¶\n$ `Dist. CBD`        &lt;dbl&gt; 19909.90, 19909.90, 19909.90, 19909.90, 19909.90, 1‚Ä¶\n$ `Top Univ.`        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ `High School`      &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ‚Ä¶\n$ `Sex Ratio`        &lt;dbl&gt; 97.83177, 97.83177, 97.83177, 97.83177, 97.83177, 9‚Ä¶\n$ Population         &lt;dbl&gt; 31022, 31022, 31022, 31022, 31022, 31022, 31022, 33‚Ä¶\n$ `Pop. Density`     &lt;dbl&gt; 1637.045, 1637.045, 1637.045, 1637.045, 1637.045, 1‚Ä¶\n$ `Higher Degree`    &lt;dbl&gt; 46.39234, 46.39234, 46.39234, 46.39234, 46.39234, 4‚Ä¶\n$ `Young Population` &lt;dbl&gt; 26.28457, 26.28457, 26.28457, 26.28457, 26.28457, 2‚Ä¶\n$ `Median Age`       &lt;dbl&gt; 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.‚Ä¶\n$ `Old Population`   &lt;dbl&gt; 5.712075, 5.712075, 5.712075, 5.712075, 5.712075, 5‚Ä¶\n$ Spring             &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, ‚Ä¶\n$ Fall               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, ‚Ä¶\n$ Winter             &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, ‚Ä¶\n\n\n\n\nWe check for any columns with missing data using ExpData() from SmartEDA package. There are no missing data in this dataset.\n\nProperty_data %&gt;%\n  ExpData(type=2)\n\n   Index    Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1  Property Prices       numeric    52644             0              0\n2      2        Longitude       numeric    52644             0              0\n3      3         Latitude       numeric    52644             0              0\n4      4             Size       numeric    52644             0              0\n5      5            Floor       numeric    52644             0              0\n6      6    Highest floor       numeric    52644             0              0\n7      7            Units       numeric    52644             0              0\n8      8          Parking       numeric    52644             0              0\n9      9          Heating       numeric    52644             0              0\n10    10             Year       numeric    52644             0              0\n11    11      Dist. Green       numeric    52644             0              0\n12    12      Dist. Water       numeric    52644             0              0\n13    13      Green Index       numeric    52644             0              0\n14    14     Dist. Subway       numeric    52644             0              0\n15    15         Bus Stop       numeric    52644             0              0\n16    16        Dist. CBD       numeric    52644             0              0\n17    17        Top Univ.       numeric    52644             0              0\n18    18      High School       numeric    52644             0              0\n19    19        Sex Ratio       numeric    52644             0              0\n20    20       Population       numeric    52644             0              0\n21    21     Pop. Density       numeric    52644             0              0\n22    22    Higher Degree       numeric    52644             0              0\n23    23 Young Population       numeric    52644             0              0\n24    24       Median Age       numeric    52644             0              0\n25    25   Old Population       numeric    52644             0              0\n26    26           Spring       numeric    52644             0              0\n27    27             Fall       numeric    52644             0              0\n28    28           Winter       numeric    52644             0              0\n   No_of_distinct_values\n1                   2562\n2                   2390\n3                   2386\n4                   5048\n5                     77\n6                     63\n7                    715\n8                    228\n9                      2\n10                    46\n11                  4631\n12                  3059\n13                  2398\n14                  2404\n15                    61\n16                  3942\n17                    27\n18                    31\n19                   382\n20                   380\n21                   381\n22                   380\n23                   381\n24                   112\n25                   381\n26                     2\n27                     2\n28                     2\n\n\n\n\nWe check for presence of duplicated records using duplicated(). The results show that there are no duplicated records.\n\nProperty_data[duplicated(Property_data),]\n\n# A tibble: 0 √ó 28\n# ‚Ñπ 28 variables: Property Prices &lt;dbl&gt;, Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;,\n#   Size &lt;dbl&gt;, Floor &lt;dbl&gt;, Highest floor &lt;dbl&gt;, Units &lt;dbl&gt;, Parking &lt;dbl&gt;,\n#   Heating &lt;dbl&gt;, Year &lt;dbl&gt;, Dist. Green &lt;dbl&gt;, Dist. Water &lt;dbl&gt;,\n#   Green Index &lt;dbl&gt;, Dist. Subway &lt;dbl&gt;, Bus Stop &lt;dbl&gt;, Dist. CBD &lt;dbl&gt;,\n#   Top Univ. &lt;dbl&gt;, High School &lt;dbl&gt;, Sex Ratio &lt;dbl&gt;, Population &lt;dbl&gt;,\n#   Pop. Density &lt;dbl&gt;, Higher Degree &lt;dbl&gt;, Young Population &lt;dbl&gt;,\n#   Median Age &lt;dbl&gt;, Old Population &lt;dbl&gt;, Spring &lt;dbl&gt;, Fall &lt;dbl&gt;, ‚Ä¶\n\n\n\n\n\n\n\n3 Data Wrangling\n\n3.1 Create Seasons Variable\nThe season of transaction is currently encoded as dummy variables in 3 separate columns. Thus, We create a new seasons variable to consolidate the data into a single column, using ifelse statements.\n\nProperty_data$Season &lt;- ifelse(Property_data$Spring==1, \"Spring\", ifelse(Property_data$Fall==1, \"Fall\", ifelse(Property_data$Winter==1, \"Winter\", \"Summer\")))\n\nProperty_data$Season &lt;- factor(Property_data$Season, \n                               levels = c(\"Spring\", \"Summer\", \"Fall\", \"Winter\"), ordered = TRUE )\n\n\n\n3.2 Bin House Size\nAs we have many continuous variables in the dataset, it will be useful to bin some of the continuous variables into categorical variables for further analyses in our project. The distribution of the size of the houses in the dataset is shown in the histogram below. Majority of the houses have size of less than 100m2, with a relatively small number having size greater than 130m2.\n\n\nShow the code\nggplot(Property_data, aes(Size)) +\n  geom_histogram(boundary = 100,\n                 color=\"black\", \n                 fill=\"#7F948F\") +\n  labs(title = \"Frequency of Size\", x=\"Size (m\\u00B2)\")+\n  scale_x_continuous(n.breaks = 20)\n\n\n\n\n\n\n\n\n\nIn a figure from the National Atlas of Korea on gross floor area by housing type, the floor area has been categorised into 5 categories and hence we will bin the house sizes similarly as well: (1) Below 40m2 (2) 40-&lt;60m2(3) 60-&lt;85m2 (4) 85-&lt;130m2 (5) 130m2 and above\n\n\nProperty_data$Size_binned &lt;- ifelse(Property_data$Size&lt;40, \"&lt;40\", ifelse(Property_data$Size&lt;60, \"40-&lt;60\", ifelse(Property_data$Size&lt;85, \"60-&lt;85\", ifelse(Property_data$Size&lt;130, \"85-&lt;130\", \"\\u2265 130\"))))\n\n\n\n3.3 Bin House Floor\nThe distribution of the house floors are shown in the histogram below. Majority of the houses are below 40 floors.\n\n\nShow the code\nggplot(Property_data, aes(Floor)) +\n  geom_histogram(boundary = 100,\n                 color=\"black\", \n                 fill=\"#7F948F\") +\n  labs(title = \"Frequency of Floor\")+\n  scale_x_continuous(n.breaks = 20)\n\n\n\n\n\n\n\n\n\nBased on the graph, we could categorise the floor levels into: (1) Low: &lt;6 (2) Middle: 6-15 (3) Middle-high: 16-25 (4) High: 26-40 (5) Top: &gt;40\n\nProperty_data$Floor_binned &lt;-ifelse(Property_data$Floor&lt;6,\"Low\", ifelse(Property_data$Floor&lt;16,\"Middle\",ifelse(Property_data$Floor&lt;26,\"Middle-high\", ifelse(Property_data$Floor&lt;41,\"High\", \"Top\"))))\n\n\n\n3.3 Bin House Construction Year\nThe year of construction of the houses transacted in the dataset ranges from 1969 to 2019. We will bin the years: (1) 1960s to 1970s: 1969 - 1979 (2) 1980s to 1990s: 1980-1999 (3) 2000s: 2000-2009 (4) 2010s: 2010-2019\n\nProperty_data$Year_binned &lt;-ifelse(Property_data$Year&lt;1980, \"1969-1979\",ifelse(Property_data$Year&lt;2000, \"1980-1999\", ifelse(Property_data$Year&lt;2010, \"2000-2009\", \"2010-2019\")))\n\n\n\n\n4 EDA\n\nDistributions of Environmental Amenities and Local Built Environments\nFirstly, we will generate boxplots and display smoothed histogram to visualise the distributions of the continuous variables in the dataset. In particular, we will look at the distributions of the environmental amenities and local built environments as part of the focus of our project. The environmental factors included in our dataset includes:\n\nDist. Green: Log-transformed network distance to the nearest park, hill, or mountain in meters\nDist. Water: Log-transformed network distance to the nearest river, stream, pond, or seashore in meters\nGreen Index: Degree of street greenness exposed to pedestrians\nDist. Subway: Log-transformed network distance to the nearest subway station in meters\nBus Stop: Number of bus stops within a 400-meter radius of a property\nHigh School: Number of high schools within a 5-km radius of a property\n\n\n\nShow the code\np1&lt;-ggplot(Property_data, \n       aes(x = `Bus Stop`, \n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(y=\"Density\")\n\np2&lt;-ggplot(Property_data, \n       aes(x = `Dist. Green`, \n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(y=\"Density\")\n\np3&lt;-ggplot(Property_data, \n       aes(x = `Dist. Water`, \n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(y=\"Density\")\n\np4&lt;-ggplot(Property_data, \n       aes(x = `Green Index`, \n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(y=\"Density\")\np5&lt;-ggplot(Property_data, \n       aes(x = `Dist. Subway`, \n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(y=\"Density\")\n\np6&lt;-ggplot(Property_data, \n       aes(x = `High School`, \n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(y=\"Density\")\n(p1+p2)/(p3+p4)/(p5+p6)\n\n\n\n\n\n\n\n\n\nFrom the plots above, we can see that:\n\nBus Stop: The distribution is right-skewed, with the majority of houses having fewer than 20 bus stops within a 400-meter radius. While most houses are located near at least one bus stop, which can enhance accessibility, a higher number of bus stops in proximity may also increase exposure to pollution from nearby roads. It would thus be interesting to see how the property prices varies with the number of bus stops in the vicinity later on through CDA.\nDist. Green (Distance to Green Area): The distribution exhibits two peaks, one around 5 and another around 9.6. Therefore, it may be beneficial for further analysis to categorize this variable into two bins based on these two peaks.\nDist. Water (Distance to nearest river, stream, pond, or seashore): The distribution is slightly left skewed.\nGreen Index: The distribution resembles a normal distribution.\nDist. Subway (Distance to Subway Station): This distribution has a concentration around 7.\nHigh School (Distance to High Schools): There are multiple peaks that resemble normal distributions. This suggests that the data might be a mixture of several normal distributions rather than a single one. This suggests that different areas may have distinct clusters of school density. Some possible explanations for this observation may include:\n\nProperties in urban areas or dense cities may have a high density of nearby schools, forming peaks at higher values. In contrast, suburban or rural areas may have fewer nearby schools, creating peaks at lower values.\nSome regions might have policies that cluster multiple schools together to form education hubs.\nAreas with physical barriers (eg rivers, mountains) may have fewer schools nearby due to the land constraints.\n\n\nFor further analysis, we can bin Bus Stop and Dist. Green. Given the large number of peaks for High School (&gt;10), categorizing this variable may oversimplify the data and obscure the underlying patterns, making it less beneficial for analysis.\nFor Bus Stop, we will bin according to this definition: (1) 0-5 (2) 6-10 (3) 11-20 (4) 21-30 (5) &gt;30\nFor Dist. Green, the 2 categories will be defined as: (1) &lt;8 (2) &gt;=8\n\nProperty_data$BusStop_binned &lt;-ifelse(Property_data$`Bus Stop`&lt;=5,\"0-5\", ifelse(Property_data$`Bus Stop`&lt;=10, \"6-10\", ifelse(Property_data$`Bus Stop`&lt;=20, \"11-20\", ifelse(Property_data$`Bus Stop`&lt;=30, \"21-30\", \"&gt;30\"))))\n\nProperty_data$BusStop_binned &lt;- factor(Property_data$BusStop_binned, \n                               levels = c(\"0-5\", \"6-10\", \"11-20\", \"21-30\", \"&gt;30\"), ordered = TRUE )\n\nProperty_data$DistGreen_binned &lt;-ifelse(Property_data$`Dist. Green`&lt;8, \"&lt;8\", \"&gt;=8\")\n\n\n\nShow the code\nProperty_data$Floor_binned &lt;- factor(Property_data$Floor_binned, \n                               levels = c(\"Low\", \"Middle\", \"Middle-high\", \"High\", \"Top\"), ordered = TRUE )\n\np1&lt;-ggplot(Property_data, \n       aes(\n           y = `Property Prices`,\n           fill=`Property Prices`\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"Floor\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Property price increases with floor\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\n\n\nProperty Price and Floor\nThe team hopes to enable users to visualise how the distributions of various continuous variables in the dataset may differ if grouped by categorical variables as well. Based on common knowledge and general trends in Singapore housing prices, house prices varies across floor ranges, with houses on higher floors typically fetching higher transaction prices compared to low floors. Therefore, we will examine the relationship between house price and floor range in Busan.\n\n\nShow the code\nProperty_data$Floor_binned &lt;- factor(Property_data$Floor_binned, \n                               levels = c(\"Low\", \"Middle\", \"Middle-high\", \"High\", \"Top\"), ordered = TRUE )\n\np1&lt;-ggplot(Property_data, \n       aes(x = Floor_binned, \n           y = `Property Prices`,\n           fill=Floor_binned\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"Floor\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Property price increases with floor\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n    stat_summary(fun.y=mean, geom=\"point\", shape=20, size=2, color=\"red4\", fill=\"red4\")+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\nBased on the plot, we observe an increasing trend in the median prices of houses in low floors to the top floors. In addition, distribution of the house prices have some resemblance to normal distribution. The median house prices for high and top floors is significantly higher than the median house prices for low floors.\nHowever, confirmatory analysis will be required to examine if the prices are significantly different between the low floor, middle floor and middle-high floors, as the median prices are only slightly different based on the plot above. Thus this could be explored by creating a another chart for visualising statistical test results, as it may be too cluttered if we add in the significance of t-tests into the plot as well, as seen below:\n\n\nShow the code\np1&lt;-ggplot(Property_data, \n       aes(x = Floor_binned, \n           y = `Property Prices`,\n           fill=Floor_binned\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"Floor\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Property price increases with floor\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n    stat_summary(fun.y=mean, geom=\"point\", shape=20, size=2, color=\"red4\", fill=\"red4\")+\n  geom_pwc(\n  aes(group = Floor_binned), tip.length = 0,\n  method = \"t_test\", label = \"p.adj.signif\"\n)+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\nFor the shiny application, we can add in the following features to enhance interactivity:\n\nenable users to choose which variables they want to visualise. They can choose either 1 continuous variable only or 1 categorical and 1 continuous variable\nsince plotly does not support stathalfeye, we cannot display summary statistics using interactive plot tool tip. Therefore we can place panels to indicate the summary statistics such as mean/median values for the continuous variable (Overall) instead and an additional table can be included at the side to display summary statistics grouped by the categorical variable\nenable users to filter the range for the continuous variable\n\n\n\nProperty Price and Bus Stops\nNext, we examine if property price varies with number of bus stops in the vicinity.\n\n\nShow the code\np1&lt;-ggplot(Property_data, \n       aes(x = BusStop_binned, \n           y = `Property Prices`,\n           fill=BusStop_binned\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"No. of Bus Stops Nearby\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Property price does not vary with number of bus stops\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n    stat_summary(fun.y=mean, geom=\"point\", shape=20, size=2, color=\"red4\", fill=\"red4\")+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\nContrary to expectations, there appears to be only slight differences in median housing prices across the binned ranges of bus stops nearby based on the plot. However, this needs to be verified through CDA. This suggests that the proximity to bus stops may not be as influential on housing prices as initially assumed. This could be explained by the diminishing returns of accessibility: while a few bus stops may enhance convenience, the effect might level off or even become a liability as congestion or noise from busier areas increases. Further investigation into additional contextual factors or finer grained analysis might be needed to uncover the true relationship between public transportation access and housing prices.\n\n\nProperty Price and Dist. Green\nNext, we examine if property price varies with distance to greenery.\n\n\nShow the code\np1&lt;-ggplot(Property_data, \n       aes(x = DistGreen_binned, \n           y = `Property Prices`,\n           fill=DistGreen_binned\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"Distance to Greenery\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Property price does not vary significantly between DistGreen categories\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n    stat_summary(fun.y=mean, geom=\"point\", shape=20, size=2, color=\"red4\", fill=\"red4\")+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\nThe median property prices for houses closer to greenery is higher compared to houses further from greenery, as expected. However, the difference appears to be rather small based on the plot above and hence the significance of this difference needs to be verified through CDA. The small difference in housing prices could be due to the following reasons:\n\nValue of proximity to green spaces might not be directly reflected in property prices. Greenery could be valued more highly in specific contexts (such as urban areas), while in other locations, it may not significantly affect housing prices. In addition, some properties may have limited access to greenery, but this could be compensated by other features, such as access to shopping centers, schools, and public transportation, which could reduce the impact of access to greenery on housing prices.\nThere might be a loss of important nuances or subtleties in how different distances to greenery affect housing prices by binning into just 2 categories. A more granular categorization, or examing distance to greenery as a continuous variable, might capture more detailed patterns that are missed when using broad bins.\n\n\n\nHousing Prices and Season of Transaction\nNext, we are interested to find out if the season could affect the housing transaction prices in Korea.\n\n\nShow the code\np1&lt;-ggplot(Property_data, \n       aes(x = Season, \n           y = `Property Prices`,\n           fill=Season\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"Season\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Housing Prices are significantly higher in fall\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n    stat_summary(fun.y=mean, geom=\"point\", shape=20, size=2, color=\"red4\", fill=\"red4\")+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\nBased on the plot, there is no difference in the median housing prices for transactions that occur during spring, summer and winter. However, median transaction prices during fall is significantly higher compared to other seasons. This could be due to fall season providing a more favorable weather for moving house, Unlike the hot and humid summer or the cold and snowy winter. In addition, families with schooling children may prefer to finalize home purchases in the fall so they can move in before the winter school break. Additionally, some universities and companies also have fall admissions or job relocations, contributing to higher housing demand. The combination of these factors may boost housing demand in fall, leading to upward pressure on prices.\n\n\nYear of Construction & Housing Prices\nLastly, we investigate the relationship between year of construction and housing prices.\n\n\nShow the code\np1&lt;-ggplot(Property_data, \n       aes(x = Year_binned, \n           y = `Property Prices`,\n           fill=Year_binned\n          )) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.1,\n               .width = 0,\n               point_colour = NA,\n                position = position_dodge(0.1)) +\n  geom_boxplot(width = .2,\n               position = position_dodge(0.3),\n               outlier.shape = NA)+\n  labs(x = \"Year of Construction\",\n       y = \"Property Price (won/m\\u00B2)\",\n       title =\"Houses built in 1969-1979 have higher transaction values\") +   \n  theme(\n    legend.position = \"right\",\n    axis.title.y = element_text(hjust=1, angle=0)) +\n  coord_flip()+ scale_fill_brewer(palette=\"Pastel1\")+\n    stat_summary(fun.y=mean, geom=\"point\", shape=20, size=2, color=\"red4\", fill=\"red4\")+\n  theme(axis.title.y = element_text(angle = 90,hjust = 0.5)) \np1\n\n\n\n\n\n\n\n\n\nInterestingly, houses built between 1969 and 1979 have significantly higher transaction prices compared to those built in later decades, while properties from 1980 to 1999 have the lowest median prices. Homes constructed between 2000 and 2009, as well as those from 2010 to 2019, exhibit relatively similar price levels. This trend could be attributed to several factors. Older properties from 1969-1979 may have heritage or architectural significance or hold potential for redevelopment, driving up their market prices as investors anticipate future gains. In contrast, houses built between 1980 and 1999 may fall into a transitional phase where they lack both the charm and of older homes and the modern amenities of newer ones, leading to lower demand and subsequently lower prices. Meanwhile, the homes built after 2000 may have similar features and hence exhibit little variation in housing prices.\n\n\nParallel coordinates plot with histogram\nAs we have many continuous variables in the dataset, it may be useful to explore the use of parallel coordinates plot with histogram.\n\n\nShow the code\ncontinuous_vars&lt;-c(\"Property Prices\", \"Floor\", \"Parking\", \"Year\", \"Dist. Green\", \"Dist. Water\", \"Green Index\", \"Dist. Subway\", \"Bus Stop\", \"High School\" )\ncont_df&lt;-Property_data[,continuous_vars]\n# Normalizing the data to [0, 1] range for each column in the data frame\ndf_normalized &lt;- apply(cont_df, 2, function(x) (x - min(x)) / (max(x) - min(x)))\n\n# Convert the result back to a data frame \ndf_normalized &lt;- as.data.frame(df_normalized)\n\n\nhistoVisibility &lt;- rep(TRUE, ncol(df_normalized[1:3]))\nparallelPlot(df_normalized[1:3],\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)\n\n\n\n\n\n\nThe parallel coordinate plot with histogram allows for the simultaneous display of multiple continuous variables, with each variable represented by a vertical axis. Observations are depicted as lines that traverse across the axes By examining the path of each line across the axes, we can easily track how changes in one variable influence others and possibly identify patterns. However, if too many variables are selected, the interactive plot will hang and may be difficult to interpret.\nHence, for our shiny app, we could set a minimum selection of 3 variables and up to 5 variables can be selected.\n\n\n\n5 CDA\n\n5.1 Bivariate Analysis: Continuous Variables\nNext, our team is interested in finding out if there is any significant correlations between the continuous variables in the dataset, particularly between housing prices and the environmental factors.\n\n\nShow the code\ncontinuous_vars&lt;-c(\"Property Prices\", \"Floor\", \"Parking\", \"Dist. Green\", \"Dist. Water\", \"Green Index\", \"Dist. Subway\", \"Bus Stop\", \"High School\", \"Year\")\ncont_df&lt;-Property_data[,continuous_vars]\nggstatsplot::ggcorrmat(\n  data = cont_df\n)\n\n\n\n\n\n\n\n\n\nBased on the correlation heatmap, housing prices do not exhibit a strong linear relationship with any of the environmental factors, as the absolute correlation values remain below 0.5. This suggests that variables such as distance to greenery, water bodies, subway stations, bus stops, and high schools have a more complex influence on property prices that cannot be captured solely through linear correlations. It is possible that these factors interact in a non-linear manner or are influenced by additional contextual variables such as neighborhood desirability or government policies. In contrast, variables like parking space availability, floor level, and construction year show stronger linear relationships with housing prices, indicating that these factors may play a more direct role in determining property values. Several potential reasons could explain this observation:\n\nHousing prices are influenced by combination of multiple factors. Environmental factors like proximity to greenery might not be the primary drivers of price variations.\nThe correlation analysis measures only linear relationships between variables. If the relationship between housing prices and environmental factors is non-linear this may not be captured. For example, being too close to a bus stop or subway station could increase accessibility but also expose residents to noise and pollution, leading to a mixed effect on property values.\nParking space, floor and construction year may have more direct impact on households. Parking space is a practical necessity, particularly in urban areas where parking is limited, making it a strong determinant of property value. Higher floors often offer better views, more privacy, and less noise, making them more desirable. Construction year reflects the age, design, and structural quality of a property. Newer buildings tend to have modern amenities and better infrastructure, making them more expensive.\n\nFor the shiny application, we can add in the following features to enhance interactivity:\n\nenable users to select which continuous variables they wish to include in the correlation heatmap. Users have to select a minimum of 3 variables.\nenable users to choose the type of statistical approach: ‚Äúparametric‚Äù, ‚Äúnonparametric‚Äù, ‚Äúrobust‚Äù and ‚Äúbayes‚Äù.\nenable users to select the significance level\nenable users to choose the p.adjust.method\n\nTo visualise the relationship and see the p-value, we can incorporate the scatterplot diagram into the shiny application alongside the correlation heatmap. The scatterplot may be useful for complementing the correlation heatmap, as we can possibly add additional panel control in the shiny app to enable users to filter the variable by another categorical variable, or by the range of a continuous variable. This may enable us to better visualise the complex relationships between housing prices and the various factors. Below, we show the scatterplot of property prices and green index.\n\n\nShow the code\nggscatterstats(\n  data = Property_data,\n  x = `Property Prices`,\n  y = `Green Index`,\n  marginal = TRUE,\n  )\n\n\n\n\n\n\n\n\n\nA weak negative correlation is observed between the green index and property prices, suggesting that homes in greener areas tend to be less expensive. This may be because urban areas, where prices are higher, are more developed and have fewer green spaces, while suburban and rural areas have more greenery but lower demand, fewer amenities, and greater distance from city centers. Additionally, green spaces are often far from commercial hubs and transport links, making accessibility a higher priority for buyers. Areas with high Green Index values may also be zoned for parks, nature reserves, or low-density housing, which generally have lower real estate values. In contrast, prime urban properties may have limited greenery but command higher prices due to location advantages. The weak correlation may also be due to the Green Index not capturing green space quality‚Äîfor instance, large undeveloped areas far from key facilities may add little value, whereas well-maintained parks in prime locations can drive up housing prices.\nFor the shiny application, we can add in the following features to enhance interactivity:\n\nenable users to select 2 continuous variables to visualise on the scatterplot\nenable users to choose the type of statistical approach: ‚Äúparametric‚Äù, ‚Äúnonparametric‚Äù, ‚Äúrobust‚Äù and ‚Äúbayes‚Äù.\nenable users to filter by range of the continuous variables selected for the scatter plot, enabling the visualization of more complex relationships beyond simple linear trends. Grouping by a categorical variable can reveal additional insights.\n\n\n\n5.2 Bivariate Analysis: Continuous & Categorical Variables\nIn this section, we will do confirmatory analyses for the findings observed in EDA (section 4).\n\nProperty Price and Floor\n\n\nShow the code\nggbetweenstats(\n  data = Property_data,\n  x = Floor_binned, \n  y = `Property Prices`,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nBased on the results, we can see that there is indeed an increasing trend of median house price, from low to top floors.\nFor the shiny application, we can add in the following features to enhance interactivity:\n-enable users to choose 1 continuous variable and 1 categorical variable - enable users to choose the type of statistical approach: ‚Äúparametric‚Äù, ‚Äúnonparametric‚Äù, ‚Äúrobust‚Äù and ‚Äúbayes‚Äù. - enable users to select which pairwise comparisons to display: (1) ‚Äúsignificant‚Äù (abbreviation accepted: ‚Äús‚Äù), (2) ‚Äúnon-significant‚Äù (abbreviation accepted: ‚Äúns‚Äù), (3) ‚Äúall‚Äù. - enable users to choose the p.adjust.method - may be usedul to show a table displaying all the statistic results\n\n\nProperty Price and Bus Stops\n\n\nShow the code\nggbetweenstats(\n  data = Property_data,\n  x = BusStop_binned, \n  y = `Property Prices`,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nThe plot shows that houses with 6-10 bus stops nearby have consistently higher prices compared to those with 0-5 bus stops or more than 10 bus stops. This suggests that having 6-10 bus stops strikes an optimal balance between accessibility and convenience, making these properties more desirable. In contrast, fewer than 6 bus stops may indicate limited public transport options, while more than 10 bus stops might not add further value‚Äîpotentially due to increased noise, congestion, or diminishing returns in accessibility.\n\n\nProperty Price and Dist. Green\n\n\nShow the code\nggbetweenstats(\n  data = Property_data,\n  x = DistGreen_binned, \n  y = `Property Prices`,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nProperty prices are indeed significantly higher for houses nearer to greenery.\n\n\nHousing Prices and Season of Transaction\n\n\nShow the code\nggbetweenstats(\n  data = Property_data,\n  x = Season, \n  y = `Property Prices`,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nApart from fall having higher transaction prices compared to all other seasons, the statistic test also revealed that spring and summer have lower transaction prices than winter, but there are no significant differences in transaction prices in spring and summer.\n\n\nYear of Construction & Housing Prices\n\n\nShow the code\nggbetweenstats(\n  data = Property_data,\n  x = Year_binned, \n  y = `Property Prices`,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nHouses built in 1980-1999, 2000-2009 and 2010-2019 folow trend of increasing house prices for newer houses. However, house built in 1969-1979 is significant more valued compared to the newer houses.\n\n\n\n\n6 Storyboard\n\nEDA Module\nWe will create the Boxplot + raincloud plot as shown below, consisting of 2 possible views:\n(1) When one continuous variable is selected only\n\n(2) When 1 continuous variable + 1 categorical variable is selected:\n\nIn addition, we will have a separate tab featuring the parallel coordinate plot with histogram:\n\n\n\nCDA Module\nWe will create 2 tabs for CDA. The first tab will feature the correlation heatmap and scatterplot:\n\nThe second tab will feature the violin-boxplot for statistical tests between groups:\n\n\n\n\n7 Saving the cleaned dataset\nWe will save the final cleaned dataset for use in the shiny app.\n\nwrite.csv(Property_data, \"Property_data_cleaned.csv\", row.names = FALSE)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nGreen Spaces and Real Estate: Exploring the Influence of Environmental Factors on Housing Prices in Busan\n",
    "section": "",
    "text": "Welcome to isss608 Group Project\nWelcome to the Group5 project homepage! The primary objective of this project is to investigate how various factors, with a greater focus on the environmental amenities and the built environment factors, influences property prices in Korea. The project seeks to understand whether properties located closer to green spaces and other amenities tend to have higher market values. The secondary objective is to explore the interaction between green amenities, the built environment, and other factors, such as neighborhood demographics and seasonality, in shaping property prices.\n\n\nProject Overview\nIn this project, we integrate various state-of-the-art data analysis methods‚Äîfrom Exploratory Data Analysis (EDA) and Confirmatory Data Analysis (CDA) to Explanatory Modeling and Predictive Modeling. Our comprehensive analytical approach is designed to reveal the underlying value of the data, providing actionable insights that can inform decision-making and drive practical applications.\n\nbackground photo sources: canva\n\n\n\nShiny App\n\n  Go explore\n\n\n\nPrototype\n\n  Go explore"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3a.html#introduction",
    "href": "Prototype/Explanatory/Take-home_Ex3a.html#introduction",
    "title": "LCA Model Design",
    "section": "1. Introduction",
    "text": "1. Introduction\nLatent Class Analysis (LCA) offers an effective methodology for segmenting the property market by identifying unobserved subgroups within the data. This approach enables us to discover distinct property types based on multiple categorical variables rather than relying solely on price ranges. The analysis will help understand the different market segments that exist and the key characteristics that define each segment.\nThe primary research questions include: 1. What distinct property market segments exist based on property and location characteristics? 2. Which variables are most important in defining these market segments? 3. How can these segments inform property development and marketing strategies?"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3a.html#r-packages-assessment-and-selection",
    "href": "Prototype/Explanatory/Take-home_Ex3a.html#r-packages-assessment-and-selection",
    "title": "LCA Model Design",
    "section": "2. R Packages Assessment and Selection",
    "text": "2. R Packages Assessment and Selection\n\npoLCA: Core package for latent class analysis implementation\ntidyverse: For data preparation and categorical variable creation\nggplot2: For visualization of class probabilities and model comparison\nmclust: For model evaluation and entropy calculation\nJustification for each package selection\n\n\n\nCode\n# Load packages\nlibrary(readxl)      # For reading Excel files\nlibrary(tidyverse)   # For data manipulation and visualization\nlibrary(poLCA)       # For latent class analysis\nlibrary(mclust)      # For model evaluation\nlibrary(DT)          # For interactive data tables\nlibrary(plotly)      # For interactive plots"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3a.html#data-preparation-and-testing",
    "href": "Prototype/Explanatory/Take-home_Ex3a.html#data-preparation-and-testing",
    "title": "LCA Model Design",
    "section": "3. Data Preparation and Testing",
    "text": "3. Data Preparation and Testing\n\nDataset Overview\nThe analysis is based on a comprehensive property price dataset containing 52,644 observations across 28 variables. The dataset includes crucial property characteristics and location-based features. Key variables in the dataset include property prices (the target variable), physical property attributes (size, floor level, highest floor in building, number of units), location-specific features (distances to green spaces, water, subway stations, CBD), environmental quality indicators (Green Index), and demographic information (population density, education levels, age distributions). The data spans properties with sizes ranging from 12.49 to 269.68 square meters, located on floors ranging from -1 to 77, with varying accessibility to amenities.\nA thorough examination of the dataset structure revealed complete data with no missing values across all variables. This completeness provided an excellent foundation for reliable latent class analysis without the need for imputation or handling of missing data. The data also displays considerable variation across all key variables, suggesting the potential for meaningful market segmentation.\n\n\nCode\n# Load data\nProperty_data &lt;- read_excel(\"data/Property_Price_and_Green_Index.xlsx\")\n\n# Examine data structure\nglimpse(Property_data)\n\n\n\n\nVariable Discretization Process\nFor effective latent class analysis, continuous variables were transformed into categorical format. This discretization process is essential as LCA requires categorical indicators. A quantile-based approach was primarily employed to ensure balanced distribution of observations across categories while preserving meaningful differentiation between property types.\nFor most variables, tertile cutpoints (33rd and 66th percentiles) were used to create three distinct categories representing low (1), medium (2), and high (3) values. This approach was applied to Size, Green Index, Subway Distance, Population Density, and Median Age variables. The rationale for using tertiles rather than quartiles or quintiles was to maintain sufficient observation counts in each category while still capturing meaningful differentiation.\nFor Floor categorization, however, a different approach was taken. Instead of percentile-based cutpoints, more intuitive fixed cutpoints were employed: low floors (‚â§5), medium floors (‚â§10), and high floors (&gt;10). This choice reflects typical building classifications and aligns with common property market segmentation practices where floor level represents a distinct feature of vertical positioning, often associated with views, noise levels, and accessibility considerations.\n\n\nFrequency Distribution Verification\nFollowing the discretization process, thorough verification of frequency distributions was conducted to ensure the effectiveness of the categorization. The frequency tables for each categorical variable confirmed generally balanced categories with sufficient observations in each group to support stable LCA estimation.\nSize categories showed excellent balance with approximately 33% of observations in each category (17,374 in category 1, 17,609 in category 2, and 17,661 in category 3). Similar balanced distributions were observed for Green Index categories, Subway Distance categories, and Population Density categories, all showing approximately equal distribution across the three levels.\nFloor categories showed a somewhat different distribution pattern, with a higher concentration of properties in category 3 (floors above 10) comprising 26,406 observations, while categories 1 and 2 contained 13,004 and 13,234 observations respectively. This distribution reflects the actual property landscape in the dataset, with a predominance of properties located on higher floors.\nMedian Age categories also showed a relatively balanced distribution (17,670 in category 1, 18,504 in category 2, and 16,470 in category 3), with a slightly higher number of properties in areas with middle-aged populations.\nThese distribution verifications confirmed that the discretization process produced meaningful categories with sufficient observations for robust LCA modeling, enabling the identification of distinct property market segments.\n\n\nCode\n# 1. Initial data checking\nstr(Property_data)\nsummary(Property_data)\n\n# 2. Check for missing values\nmissing_values &lt;- colSums(is.na(Property_data))\nprint(missing_values)\n\n# 3. Variable discretization\nProperty_data_lca &lt;- Property_data %&gt;%\n  mutate(\n    # Size categorization (based on quantiles)\n    Size_cat = case_when(\n      Size &lt;= quantile(Size, 0.33) ~ 1,\n      Size &lt;= quantile(Size, 0.66) ~ 2,\n      TRUE ~ 3\n    ),\n    \n    # Floor categorization\n    Floor_cat = case_when(\n      Floor &lt;= 5 ~ 1,\n      Floor &lt;= 10 ~ 2,\n      TRUE ~ 3\n    ),\n    \n    # Green Index categorization\n    Green_Index_cat = case_when(\n      `Green Index` &lt;= quantile(`Green Index`, 0.33) ~ 1,\n      `Green Index` &lt;= quantile(`Green Index`, 0.66) ~ 2,\n      TRUE ~ 3\n    ),\n    \n    # Distance to subway categorization\n    Subway_Dist_cat = case_when(\n      `Dist. Subway` &lt;= quantile(`Dist. Subway`, 0.33) ~ 1,\n      `Dist. Subway` &lt;= quantile(`Dist. Subway`, 0.66) ~ 2,\n      TRUE ~ 3\n    ),\n    \n    # Population density categorization\n    Pop_Density_cat = case_when(\n      `Pop. Density` &lt;= quantile(`Pop. Density`, 0.33) ~ 1,\n      `Pop. Density` &lt;= quantile(`Pop. Density`, 0.66) ~ 2,\n      TRUE ~ 3\n    ),\n    \n    # Median Age categorization\n    Median_Age_cat = case_when(\n      `Median Age` &lt;= quantile(`Median Age`, 0.33) ~ 1,\n      `Median Age` &lt;= quantile(`Median Age`, 0.66) ~ 2,\n      TRUE ~ 3\n    )\n  )\n\n# Check frequency distribution of categorical variables\nlca_vars &lt;- c(\"Size_cat\", \"Floor_cat\", \"Green_Index_cat\", \"Subway_Dist_cat\", \n              \"Pop_Density_cat\", \"Median_Age_cat\")\n\nfor(var in lca_vars) {\n  cat(\"\\nFrequency table for\", var, \":\\n\")\n  print(table(Property_data_lca[[var]]))\n}"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3a.html#lca-model-fitting-and-evaluation",
    "href": "Prototype/Explanatory/Take-home_Ex3a.html#lca-model-fitting-and-evaluation",
    "title": "LCA Model Design",
    "section": "4. LCA Model Fitting and Evaluation",
    "text": "4. LCA Model Fitting and Evaluation\n\n\nCode\n# Create formula\nf &lt;- cbind(Size_cat, Floor_cat, Green_Index_cat, Subway_Dist_cat, \n           Pop_Density_cat, Median_Age_cat) ~ 1\n\n# Set range of classes from 3 to 8\nmin_classes &lt;- 3\nmax_classes &lt;- 8\n\n# Store model results\nlca_results &lt;- list()\nlca_stats &lt;- data.frame(n_classes = min_classes:max_classes)\n\n# Entropy calculation function\nentropy &lt;- function(posterior) {\n  entropy &lt;- -rowSums(posterior * log(posterior), na.rm = TRUE)\n  max_entropy &lt;- -log(1/ncol(posterior))\n  return(1 - mean(entropy) / max_entropy)\n}\n\nset.seed(123)  # For reproducibility\n\n# Previous BIC value for early stopping\nprev_bic &lt;- Inf\nconsecutive_increases &lt;- 0\nmax_consecutive_increases &lt;- 2  # Stop after 2 consecutive BIC increases\n\nfor (i in min_classes:max_classes) {\n  cat(\"\\nFitting model with\", i, \"classes...\\n\")\n  \n  # Optimize for speed\n  lca_results[[i]] &lt;- poLCA(f, Property_data_lca, nclass = i, \n                           maxiter = 1000, nrep = 10)\n  \n  # Store fit statistics\n  idx &lt;- i - min_classes + 1\n  lca_stats$logLik[idx] &lt;- lca_results[[i]]$llik\n  lca_stats$BIC[idx] &lt;- lca_results[[i]]$bic\n  lca_stats$AIC[idx] &lt;- lca_results[[i]]$aic\n  lca_stats$df[idx] &lt;- lca_results[[i]]$resid.df\n  lca_stats$Entropy[idx] &lt;- entropy(lca_results[[i]]$posterior)\n  \n  # Print summary information\n  cat(\"Log-likelihood:\", lca_results[[i]]$llik, \"\\n\")\n  cat(\"BIC:\", lca_results[[i]]$bic, \"\\n\")\n  cat(\"AIC:\", lca_results[[i]]$aic, \"\\n\")\n  \n  # Early stopping check\n  current_bic &lt;- lca_results[[i]]$bic\n  if (current_bic &gt; prev_bic) {\n    consecutive_increases &lt;- consecutive_increases + 1\n    cat(\"BIC increased. Consecutive increases:\", consecutive_increases, \"\\n\")\n    \n    if (consecutive_increases &gt;= max_consecutive_increases) {\n      cat(\"\\nEarly stopping triggered after\", consecutive_increases, \n          \"consecutive BIC increases. Optimal classes may be\", i - consecutive_increases, \"\\n\")\n      break\n    }\n  } else {\n    consecutive_increases &lt;- 0\n  }\n  \n  prev_bic &lt;- current_bic\n}\n\n# Handle the case where we stopped early\nactual_max &lt;- if(exists(\"i\")) min(i, max_classes) else max_classes\nlca_stats &lt;- lca_stats[1:(actual_max - min_classes + 1), ]\n\n# Select best model based on BIC\nbest_model_idx &lt;- which.min(lca_stats$BIC) + min_classes - 1\ncat(\"\\nBest model based on BIC has\", best_model_idx, \"classes.\\n\")\n\n# Visualize model comparison\nggplot(lca_stats, aes(x = n_classes)) +\n  geom_line(aes(y = BIC, color = \"BIC\")) +\n  geom_point(aes(y = BIC, color = \"BIC\")) +\n  geom_line(aes(y = AIC, color = \"AIC\")) +\n  geom_point(aes(y = AIC, color = \"AIC\")) +\n  labs(title = \"LCA Model Comparison\", \n       x = \"Number of Classes\", \n       y = \"Information Criteria\") +\n  theme_minimal()\n\n\n\n\nCode\n# After finding best model, save results\nbest_model &lt;- lca_results[[best_model_idx]]\n\n# 1. Save the entire model object\nsaveRDS(best_model, \"best_lca_model.rds\")\n\n# 2. Save the class assignments for easier access\nclass_assignments &lt;- apply(best_model$posterior, 1, which.max)\nwrite.csv(data.frame(id = 1:length(class_assignments), \n                    class = class_assignments), \n         \"lca_class_assignments.csv\", row.names = FALSE)\n\n# 3. Save the model comparison statistics\nwrite.csv(lca_stats, \"lca_model_stats.csv\", row.names = FALSE)\n\n\n\nLatent Class Analysis (LCA) Results Summary\n\nModel Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Classes\nLog-likelihood\nBIC\nAIC\nG¬≤\nX¬≤\nNumber of Parameters\n\n\n\n\n3 classes\n-342045.2\n684503.5\n684166.4\n8510.915\n8480.657\n38\n\n\n4 classes\n-341613.0\n683780.4\n683327.9\n7646.455\n7608.341\n51\n\n\n5 classes\n-341273.3\n683242.4\n682674.6\n6967.175\n6990.710\n64\n\n\n6 classes\n-340902.5\n682642.2\n681959.1\n6225.598\n6076.490\n77\n\n\n7 classes\n-340525.0\n682028.5\n681230.1\n5470.593\n5492.197\n90\n\n\n8 classes\n-340291.7\nNot shown\nNot shown\nNot shown\nNot shown\nNot shown\n\n\n\nNote: All models display the warning ‚ÄúMAXIMUM LIKELIHOOD NOT FOUND‚Äù.\n\n\nClass Population Distribution\n\n3-Class Model\n\nClass 1: 12.24%\nClass 2: 10.31%\nClass 3: 77.45%\n\n\n\n4-Class Model\n\nClass 1: 52.34%\nClass 2: 28.95%\nClass 3: 8.51%\nClass 4: 10.20%\n\n\n\n5-Class Model\n\nClass 1: 9.79%\nClass 2: 25.23%\nClass 3: 11.51%\nClass 4: 7.32%\nClass 5: 46.15%\n\n\n\n6-Class Model\n\nClass 1: 11.63%\nClass 2: 34.04%\nClass 3: 7.00%\nClass 4: 11.14%\nClass 5: 21.20%\nClass 6: 15.00%\n\n\n\n7-Class Model\n\nClass 1: 9.62%\nClass 2: 40.87%\nClass 3: 12.53%\nClass 4: 9.13%\nClass 5: 7.89%\nClass 6: 13.21%\nClass 7: 6.77%\n\n\n\n\nKey Variables in the Analysis\nThe LCA examined the following categorical variables: - Size_cat - Floor_cat - Green_Index_cat - Subway_Dist_cat - Pop_Density_cat - Median_Age_cat\nEach variable was divided into three categories (Pr(1), Pr(2), Pr(3)), and the conditional probabilities for these categories across different latent classes were calculated.\n\n\nModel Selection Insights\nThe model fit improves (lower BIC and AIC values) as the number of classes increases. The 7-class model shows the best fit among the fully reported models, with: - Log-likelihood: -340525.0 - BIC: 682028.5 - AIC: 681230.1\nHowever, all models show the warning ‚ÄúMAXIMUM LIKELIHOOD NOT FOUND,‚Äù which suggests potential convergence issues in the estimation process."
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3a.html#parameters-and-outputs-determination",
    "href": "Prototype/Explanatory/Take-home_Ex3a.html#parameters-and-outputs-determination",
    "title": "LCA Model Design",
    "section": "5. Parameters and Outputs Determination",
    "text": "5. Parameters and Outputs Determination\n\nVariables Selection for Latent Class Analysis\n\nPrimary Variables\nFor the latent class analysis model, six categorical variables were selected based on their statistical properties and relevance to property market segmentation:\n\nSize_cat: Property size categories derived from the continuous variable ‚ÄúSize‚Äù (ranging from 12.49 to 269.68 square meters). The discretization created balanced categories with 17,374, 17,609, and 17,661 observations across the three groups, providing sufficient data for robust class identification.\nFloor_cat: Floor level categories divided into low (‚â§5), medium (‚â§10), and high (&gt;10) floors, resulting in 13,004, 13,234, and 26,406 observations respectively. Though slightly imbalanced toward higher floors, this categorization reflects the actual distribution within the dataset.\nGreen_Index_cat: Environmental quality measure (original range: 4.163 to 18.927) categorized into three balanced groups containing 17,374, 17,404, and 17,866 observations, ensuring stable parameter estimation.\nSubway_Dist_cat: Distance to nearest subway station (original range: 3.366 to 9.978) transformed into three approximately equal categories with 17,384, 17,395, and 17,865 observations, providing consistent measurement of accessibility.\nPop_Density_cat: Population density (original range: 1 to 118,182) discretized into three categories with 17,521, 17,271, and 17,852 observations, offering balanced representation of neighborhood density characteristics.\nMedian_Age_cat: Demographic indicator (original range: 32.7 to 55.4) categorized into three groups with 17,670, 18,504, and 16,470 observations, maintaining sufficient data for reliable parameter estimation.\n\nThese variables were retained in the final model as evidenced by their inclusion in the LCA formula:\n\n\nCode\nf &lt;- cbind(Size_cat, Floor_cat, Green_Index_cat, Subway_Dist_cat, Pop_Density_cat, Median_Age_cat) ~ 1\n\n\n\n\n\nModel Selection Criteria\nTo determine the optimal number of latent classes, multiple evaluation criteria were employed:\n\nBayesian Information Criterion (BIC): This was established as the primary selection criterion due to its balanced approach to model fit and complexity. BIC imposes a stronger penalty for model complexity compared to AIC, helping to avoid overfitting. In the analysis, BIC values were calculated for models with different class numbers, ranging from 684,503.5 for the 3-class model to 682,028.5 for the 7-class model, with lower values indicating better model fit adjusted for complexity.\nAkaike Information Criterion (AIC): Used as a secondary criterion, AIC applies a less severe penalty for model complexity. AIC values ranged from 684,166.4 for the 3-class model to 681,230.1 for the 7-class model. The consistent trend in both BIC and AIC provided confidence in model selection decisions.\nEntropy: This measure was used to assess classification quality and certainty. Higher entropy values indicate clearer distinction between classes and more certain classification of observations. An entropy calculation function was implemented to evaluate how well the model separates observations into distinct classes.\nInterpretability: Beyond statistical measures, the substantive interpretability of resulting classes was considered. Classes that represented meaningful and distinct property market segments were valued over purely statistical improvements.\n\n\n\nClass Number Determination Process\nA systematic and comprehensive approach was adopted to determine the optimal number of latent classes:\n\nModels with 3 to 8 latent classes were tested sequentially. This range was chosen based on prior research suggesting that fewer than 3 classes would be insufficient to capture market complexity, while more than 8 would likely result in classes too small for meaningful interpretation.\nFor each potential class number, 10 random starts were employed to avoid local maxima in the likelihood function. This approach increases confidence that the global maximum likelihood solution was found for each model specification.\nAn early stopping mechanism was implemented to terminate testing when BIC increased consecutively across two class solutions. This approach follows the principle that when BIC begins to increase consistently, additional classes are likely overfitting the data.\nLog-likelihood, BIC, and AIC values were carefully tracked across all model iterations to monitor convergence and model fit improvement.\nThe pattern of BIC and AIC decreases was analyzed, with attention to the point where diminishing returns in model fit improvement occurred with increasing class numbers.\n\nBased on this process, the optimal model was determined to be the 6-class solution, which showed substantial improvement in BIC (682,642.2) compared to simpler models while maintaining interpretable and meaningful class structures. While the 7-class model showed a slightly lower BIC (682,028.5), the improvement was marginal and came at the cost of less distinct and less interpretable classes.\n\n\nComputational Efficiency Considerations\nSeveral strategies were implemented to enhance computational efficiency while maintaining model quality:\n\nMaximum iterations for each model were set to 1,000, striking a balance between ensuring convergence and limiting excessive computation time. This parameter was determined after observing that most models converged well within this limit.\nTen random starts were used for each class number to balance between finding the global maximum likelihood and computational time. This number proved sufficient to consistently find the same maximum likelihood solution across multiple runs.\nThe early stopping mechanism based on consecutive BIC increases significantly reduced computation time by avoiding unnecessary exploration of models with poorer fit.\nA random seed was set to ensure reproducibility while maintaining computational efficiency, allowing for consistent results across repeated analyses.\nFor models with larger numbers of classes, careful monitoring of log-likelihood, BIC, and AIC values across iterations verified proper convergence and solution stability.\n\nThese efficiency considerations allowed for thorough exploration of the model space while ensuring practical computation times, enabling the identification of an optimal latent class solution that effectively captures distinct property market segments."
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3a.html#ui-component-selection-for-shiny-application",
    "href": "Prototype/Explanatory/Take-home_Ex3a.html#ui-component-selection-for-shiny-application",
    "title": "LCA Model Design",
    "section": "6. UI Component Selection for Shiny Application",
    "text": "6. UI Component Selection for Shiny Application\n\nModel Configuration Components for Clustering\nBased on the provided images, the following UI components were selected for the Shiny application‚Äôs clustering functionality:\n\nNumber of Clusters Slider: A slider input ranging from 3 to a maximum of 8 clusters, with the current selection set to 4 clusters as shown in Image 1. This component allows users to intuitively select the desired granularity of market segmentation.\nData Sample Size Slider: A slider allowing users to select the percentage of data to use for analysis, ranging from 10% to 100% with a current selection at 50%. This enables users to perform exploratory analysis on smaller subsets before running the full analysis.\nVariable Selection Panel: A panel displaying pre-tested variables (Size_cat, Floor_cat, Green_Index_cat) that provide effective clustering results. The panel includes explanatory text informing users that these variables have been validated through preliminary experiments.\nRun Cluster Button: A prominently displayed green button that initiates the clustering process with the selected parameters.\n\n\n\n\nCode\n# UI code for model configuration\nui_model_config &lt;- fluidRow(\n  column(width = 4,\n    wellPanel(\n      style = \"background-color: #f5f5f5;\",\n      h3(\"Number of Clusters:\"),\n      sliderInput(\"n_clusters\", \"\", min = 3, max = 8, value = 4, step = 1),\n      \n      h3(\"Data Sample Size:\"),\n      sliderInput(\"sample_size\", \"\", min = 10, max = 100, value = 50, step = 10,\n                 labels = c(\"10%\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"100%\")),\n      \n      p(\"Percentage of data used for analysis\"),\n      \n      h3(\"Select Variables:\"),\n      tags$div(\n        style = \"border-left: 4px solid #2E8B57; padding-left: 10px;\",\n        p(\"The following variables have been pre-tested and shown to provide effective clustering results in preliminary experiments.\")\n      ),\n      \n      h4(\"Variables:\"),\n      div(\n        style = \"margin-bottom: 20px;\",\n        actionButton(\"size_cat\", \"Size_cat\", \n                    style = \"background-color: #90EE90; margin-right: 5px;\"),\n        actionButton(\"floor_cat\", \"Floor_cat\", \n                    style = \"background-color: #90EE90; margin-right: 5px;\"),\n        actionButton(\"green_index_cat\", \"Green_Index_cat\", \n                    style = \"background-color: #90EE90;\"),\n        p(\"Select Variables\")\n      ),\n      \n      actionButton(\"run_cluster\", \"Run Cluster\", \n                  style = \"background-color: #2E8B57; color: white; width: 100%; height: 50px;\")\n    )\n  )\n)\n\n\n\n\nResults Visualization Components\nThe visualization components are organized into tabbed panels for clear presentation:\n\nCluster Proportion Tab: Displays a horizontal bar chart showing the percentage of data in each cluster, with clear labels for each cluster‚Äôs proportion (e.g., ‚ÄúCluster 1: 27.8%‚Äù). Below the chart, key model fit statistics are displayed (AIC, BIC, Likelihood Ratio, Silhouette Score) in separate green panels.\n\n\n\n\nCluster Characteristics Tab: Shows a grouped bar chart depicting variable means for each cluster, allowing users to visually compare how different clusters score on key variables like Floor_cat, Green_Index_cat, and Size_cat. Each cluster is represented by a different shade of green.\n\n\n\n\nData Explorer Tab: Provides a tabular view of the raw data with filtering options, allowing users to filter by cluster, search for specific records, and control the number of rows displayed. The table shows detailed property information including prices, coordinates, size, floor, and other attributes.\n\n\n\n\nParallel Coordinates Tab: Presents a parallel coordinates plot visualizing the relationships between variables across different clusters. Each line represents a property profile, and colors differentiate between clusters, enabling users to understand typical value combinations across each segment.\n\n\n\n\nCode\n# UI code for visualization tabs\nui_visualization &lt;- fluidRow(\n  column(width = 8,\n    tabsetPanel(\n      tabPanel(\"Cluster Proportion\",\n        h1(\"Latent Class Analysis\", align = \"right\"),\n        h3(\"Percentage of Data in Each Cluster\"),\n        plotlyOutput(\"cluster_prop_plot\", height = \"300px\"),\n        br(),\n        fluidRow(\n          column(width = 3,\n            div(style = \"background-color: #F0FFF0; padding: 10px; border-radius: 5px;\",\n              h4(\"AIC\"),\n              p(\"-83,572.49\"),\n              actionButton(\"aic_info\", \"\", icon = icon(\"info\"))\n            )\n          ),\n          column(width = 3,\n            div(style = \"background-color: #F0FFF0; padding: 10px; border-radius: 5px;\",\n              h4(\"BIC\"),\n              p(\"-83,633.83\"),\n              actionButton(\"bic_info\", \"\", icon = icon(\"info\"))\n            )\n          ),\n          column(width = 3,\n            div(style = \"background-color: #F0FFF0; padding: 10px; border-radius: 5px;\",\n              h4(\"Likelihood Ratio\"),\n              p(\"0.579\"),\n              actionButton(\"lr_info\", \"\", icon = icon(\"info\"))\n            )\n          ),\n          column(width = 3,\n            div(style = \"background-color: #F0FFF0; padding: 10px; border-radius: 5px;\",\n              h4(\"Silhouette Score\"),\n              p(\"0.348\"),\n              actionButton(\"silhouette_info\", \"\", icon = icon(\"info\"))\n            )\n          )\n        )\n      ),\n      \n      tabPanel(\"Cluster Characteristics\",\n        h1(\"Latent Class Analysis\", align = \"right\"),\n        h3(\"Variable Means by Cluster\"),\n        plotlyOutput(\"var_means_plot\", height = \"400px\")\n      ),\n      \n      tabPanel(\"Data Explorer\",\n        h1(\"Latent Class Analysis\", align = \"right\"),\n        fluidRow(\n          column(width = 4,\n            selectInput(\"cluster_filter\", \"Filter by Cluster:\", \n                      choices = c(\"All\", \"1\", \"2\", \"3\", \"4\"))\n          ),\n          column(width = 4,\n            textInput(\"search_text\", \"Search:\")\n          ),\n          column(width = 4,\n            numericInput(\"rows_display\", \"Rows to Display:\", value = 10)\n          )\n        ),\n        DTOutput(\"data_table\")\n      ),\n      \n      tabPanel(\"Parallel Coordinates\",\n        h1(\"Latent Class Analysis\", align = \"right\"),\n        h3(\"Parallel Coordinates Plot by Cluster\"),\n        plotlyOutput(\"parallel_coords_plot\", height = \"400px\")\n      )\n    )\n  )\n)\n\n\n\n\nUser Interaction Design Rationale\nThe UI design follows several key principles for effective user interaction:\n\nClear Visual Hierarchy: The left sidebar contains all control elements, while the main panel presents results, creating a natural left-to-right workflow.\nGuided Analysis Path: The tab sequence (Cluster Proportion ‚Üí Characteristics ‚Üí Explorer ‚Üí Parallel Coordinates) guides users through increasingly detailed views of the clustering results.\nColor Consistency: A green color scheme is used throughout the application, with darker greens for controls and lighter greens for informational panels, creating visual harmony.\nImmediate Feedback: Key statistics are prominently displayed beneath visualizations, giving users immediate insight into model quality.\nProgressive Disclosure: Basic information is shown first, with detailed data exploration tools available in later tabs for users who need deeper analysis.\n\nThis design approach creates an intuitive, user-friendly interface that balances simplicity for novice users with sufficient depth for more experienced analysts.\n\n\nCode\n# Combined UI layout\nui &lt;- fluidPage(\n  titlePanel(\"Latent Class Analysis\"),\n  fluidRow(\n    column(width = 4, ui_model_config),\n    column(width = 8, ui_visualization)\n  )\n)\n\n# Define server logic \nserver &lt;- function(input, output, session) {\n  \n  # Reactive for running the cluster analysis\n  cluster_results &lt;- eventReactive(input$run_cluster, {\n    # Code to perform LCA with selected parameters would go here\n    # Return results object\n  })\n  \n  # Generate cluster proportion plot\n  output$cluster_prop_plot &lt;- renderPlotly({\n    # Code to create cluster proportion visualization\n  })\n  \n  # Generate variable means plot\n  output$var_means_plot &lt;- renderPlotly({\n    # Code to create variable means by cluster plot\n  })\n  \n  # Generate data table with filtering\n  output$data_table &lt;- renderDT({\n    # Code to create filtered data table\n  })\n  \n  # Generate parallel coordinates plot\n  output$parallel_coords_plot &lt;- renderPlotly({\n    # Code to create parallel coordinates plot\n  })\n}\n\n# Run the Shiny app\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3a.html#results-and-discussion",
    "href": "Prototype/Explanatory/Take-home_Ex3a.html#results-and-discussion",
    "title": "LCA Model Design",
    "section": "7. Results and Discussion",
    "text": "7. Results and Discussion\n\nCluster Comparison Analysis\nThe LCA clustering explored models with 3 to 8 clusters, with the following key findings:\n\nBIC Comparison: BIC values decreased from 684,503.5 (3-cluster model) to 682,028.5 (7-cluster model), with the rate of improvement slowing after 6 clusters.\nOptimal Cluster Solution: The 6-cluster solution (BIC: 682,642.2) provided the best balance between statistical fit and practical interpretability, with clearly differentiated property segments.\nCluster Proportions: The 6-cluster solution identified one large segment (34.04% of properties), four medium-sized segments (ranging from 11.14% to 21.20%), and one smaller niche segment (7.00%).\n\n\n\nCluster Profiles\nThe 6-cluster solution revealed distinct property segments:\n\nCluster 1 (11.63%): Large properties on high floors, representing the premium segment of the market with superior space and vertical positioning.\nCluster 2 (34.04%): Properties with balanced characteristics across all variables, forming the mainstream market segment that appeals to the broadest range of buyers.\nCluster 3 (7.00%): Medium-sized properties with high environmental quality in dense areas, creating a specialized urban-environmental niche.\nCluster 4 (11.14%): Medium to large properties with poor subway accessibility, representing areas where space compensates for location disadvantages.\nCluster 5 (21.20%): Properties in low-density areas with high environmental quality, forming a substantial ‚Äúgreen suburban‚Äù market segment.\nCluster 6 (15.00%): Smaller properties with excellent subway accessibility, representing the urban convenience segment where location advantages offset size limitations.\n\n\n\nMarket Segmentation Applications\nThe clustering results provide practical insights for property market analysis:\n\nThe identified clusters offer a data-driven framework for property classification that goes beyond simple price tiers.\nThe results reveal important trade-offs defining different market segments: size vs.¬†location, environmental quality vs.¬†density, and floor level vs.¬†accessibility.\nThe balanced distribution of cluster sizes indicates diverse market niches beyond the mainstream segment, suggesting opportunities for specialized development."
  },
  {
    "objectID": "Prototype/Explanatory/Take-home_Ex3a.html#conclusion",
    "href": "Prototype/Explanatory/Take-home_Ex3a.html#conclusion",
    "title": "LCA Model Design",
    "section": "8. Conclusion",
    "text": "8. Conclusion\n\nKey Findings\nThe LCA clustering successfully identified six distinct property market segments, each with unique characteristic combinations:\n\nPremium large, high-floor properties\nMainstream balanced-attribute properties\nUrban-environmental properties combining density with green amenities\nSpacious but less accessible properties\nGreen suburban properties with environmental quality and low density\nUrban convenience properties with excellent accessibility but limited size\n\n\n\nApplications\nThe clustering results provide practical applications for property market analysis:\n\nDevelopment planning guidance by identifying underserved market segments\nImproved targeting of marketing messages to specific property buyer segments\nBetter understanding of value drivers across different property types\n\n\n\nLimitations and Future Directions\nSome limitations of the current analysis include:\n\nThe discretization of variables into three categories may have simplified some distinctions\nThe exclusion of variables like construction year and socioeconomic indicators\n\nFuture work could enhance the analysis by: - Incorporating additional relevant variables - Applying the clustering approach to different geographical areas - Extending the analysis to include price data to quantify segment premiums"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html",
    "href": "Prototype/Predictive/Take-home_Ex3.html",
    "title": "Predictive Modeling",
    "section": "",
    "text": "In this take-home exercise, I will perform a predictive modeling analysis (Decision Tree and Random Forest) for the upcoming group project. Further result will be posted on here.\nA dataset used is from this¬†research paper¬†to investigate variables influencing property prices in Korea. The dataset comprises of comprehensive information on the property prices in the Busan Metropolitan City of South Korea for transactions in 2018 to 2019 and 27 variables that influence property prices including characteristics of the property, distance to environmental amenities and local built environments, local demographic characteristics, and season in which the transaction occurred."
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#select-variables",
    "href": "Prototype/Predictive/Take-home_Ex3.html#select-variables",
    "title": "Predictive Modeling",
    "section": "3.1 Select variables",
    "text": "3.1 Select variables\nRemove variables that are not useful for predictive modeling. Geographic data will not be used in this modeling.\n\nProperty_data &lt;- Property_data %&gt;% \n  select(!Longitude) %&gt;% \n  select(!Latitude)"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#create-summer-variable",
    "href": "Prototype/Predictive/Take-home_Ex3.html#create-summer-variable",
    "title": "Predictive Modeling",
    "section": "3.2 Create summer variable",
    "text": "3.2 Create summer variable\nI will create a Summer variable, where Summer = 1 if the property is recorded in summer (i.e., Spring = 0, Fall = 0, and Winter = 0). Since Spring, Fall, and Winter are already binary-encoded, we do not need to encode them further.\n\nProperty_data$summer &lt;- ifelse(Property_data$Spring == 0 & Property_data$Fall == 0 & Property_data$Winter == 0, 1, 0)\n\nNow check your data set again. There will be 27 variables with the new binary variable summer.\n\nglimpse(Property_data)\n\nRows: 52,644\nColumns: 27\n$ `Property Prices`  &lt;dbl&gt; 9.798127, 9.852194, 9.740969, 9.798127, 9.692767, 9‚Ä¶\n$ Size               &lt;dbl&gt; 45.0700, 38.1000, 45.0700, 38.1000, 38.1000, 45.070‚Ä¶\n$ Floor              &lt;dbl&gt; 8, 13, 6, 13, 7, 9, 6, 6, 11, 7, 9, 9, 10, 11, 2, 4‚Ä¶\n$ `Highest floor`    &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 11, 11,‚Ä¶\n$ Units              &lt;dbl&gt; 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 8, 8, 8‚Ä¶\n$ Parking            &lt;dbl&gt; 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.6‚Ä¶\n$ Heating            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ Year               &lt;dbl&gt; 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 201‚Ä¶\n$ `Dist. Green`      &lt;dbl&gt; 4.668050, 4.668050, 4.668050, 4.668050, 4.668050, 4‚Ä¶\n$ `Dist. Water`      &lt;dbl&gt; 7.092015, 7.092015, 7.092015, 7.092015, 7.092015, 7‚Ä¶\n$ `Green Index`      &lt;dbl&gt; 10.867812, 10.867812, 10.867812, 10.867812, 10.8678‚Ä¶\n$ `Dist. Subway`     &lt;dbl&gt; 5.655021, 5.655021, 5.655021, 5.655021, 5.655021, 5‚Ä¶\n$ `Bus Stop`         &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 13, 13, 13, ‚Ä¶\n$ `Dist. CBD`        &lt;dbl&gt; 19909.90, 19909.90, 19909.90, 19909.90, 19909.90, 1‚Ä¶\n$ `Top Univ.`        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ `High School`      &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ‚Ä¶\n$ `Sex Ratio`        &lt;dbl&gt; 97.83177, 97.83177, 97.83177, 97.83177, 97.83177, 9‚Ä¶\n$ Population         &lt;dbl&gt; 31022, 31022, 31022, 31022, 31022, 31022, 31022, 33‚Ä¶\n$ `Pop. Density`     &lt;dbl&gt; 1637.045, 1637.045, 1637.045, 1637.045, 1637.045, 1‚Ä¶\n$ `Higher Degree`    &lt;dbl&gt; 46.39234, 46.39234, 46.39234, 46.39234, 46.39234, 4‚Ä¶\n$ `Young Population` &lt;dbl&gt; 26.28457, 26.28457, 26.28457, 26.28457, 26.28457, 2‚Ä¶\n$ `Median Age`       &lt;dbl&gt; 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.‚Ä¶\n$ `Old Population`   &lt;dbl&gt; 5.712075, 5.712075, 5.712075, 5.712075, 5.712075, 5‚Ä¶\n$ Spring             &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, ‚Ä¶\n$ Fall               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, ‚Ä¶\n$ Winter             &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, ‚Ä¶\n$ summer             &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ‚Ä¶"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#rename-the-variable",
    "href": "Prototype/Predictive/Take-home_Ex3.html#rename-the-variable",
    "title": "Predictive Modeling",
    "section": "3.3 Rename the variable",
    "text": "3.3 Rename the variable\nIn R, it will be better for us using the column name without the blank.\n\nnames(Property_data) &lt;- gsub(\" \", \"_\", names(Property_data))"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#outlier",
    "href": "Prototype/Predictive/Take-home_Ex3.html#outlier",
    "title": "Predictive Modeling",
    "section": "4.1 Outlier",
    "text": "4.1 Outlier\nWhile decision trees are generally robust to outliers due to their non-parametric nature, it is recommended to check for outliers before training. Outliers may affect split point selection, especially when the feature range is large. In this analysis, we will inspect the data for outliers and decide on the appropriate action.\n\n\nshow the code\nnum_cols &lt;- sapply(Property_data, is.numeric)\nnumeric_data &lt;- Property_data[, num_cols]\n\nfind_outliers &lt;- function(x) {\n  stats &lt;- boxplot.stats(x)$stats  \n  lower_bound &lt;- stats[1]             \n  upper_bound &lt;- stats[5]             \n  outliers &lt;- x[x &lt; lower_bound | x &gt; upper_bound]\n  return(outliers)\n}\n\nboxplot(numeric_data, main=\"Box Plot for Numerical Data\", las=2)\n\n\n\n\n\n\n\n\n\nIt is observed that the variable Dist_CBD has an extremely large range and a right-skewed distribution. To reduce skewness and stabilize the feature‚Äôs range for better model performance, we apply a log transformation using log(X+1).\n\nProperty_data$`Dist._CBD` &lt;- log(Property_data$`Dist._CBD` + 1)\n\nNow check outlier again.\n\n\nshow the code\nnum_cols &lt;- sapply(Property_data, is.numeric)\nnumeric_data &lt;- Property_data[, num_cols]\n\nfind_outliers &lt;- function(x) {\n  stats &lt;- boxplot.stats(x)$stats  \n  lower_bound &lt;- stats[1]             \n  upper_bound &lt;- stats[5]             \n  outliers &lt;- x[x &lt; lower_bound | x &gt; upper_bound]\n  return(outliers)\n}\n\nboxplot(numeric_data, main=\"Box Plot for Numerical Data\", las=2)\n\n\n\n\n\n\n\n\n\nAfter checking the numerical variables, we found that most outliers come from the Population variable. Since Population may have a meaningful relationship with our dependent variable Property_Prices (e.g., higher population density might correlate with higher property prices), we will remain these outliers for now and evaluate their impact on the model later."
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#correlation",
    "href": "Prototype/Predictive/Take-home_Ex3.html#correlation",
    "title": "Predictive Modeling",
    "section": "4.2 Correlation",
    "text": "4.2 Correlation\nBefore building the decision tree, it is necessary to check the correlation between variables. However, we will allow users to select the variables themselves in Shiny. By default, the pre-cleaned variable will be selected.\n\n\nshow the code\ncutoff &lt;- 0.8\n\nnum_cols &lt;- sapply(Property_data, is.numeric)\nnumeric_data &lt;- Property_data[, num_cols]\n\ncorr_matrix &lt;- cor(numeric_data, use = \"complete.obs\")\n\nvar_names &lt;- names(numeric_data)\n\nfor (i in 1:(ncol(numeric_data) - 1)) {\n  for (j in (i + 1):ncol(numeric_data)) {\n    cor_val &lt;- corr_matrix[i, j]\n    if (abs(cor_val) &gt; cutoff) {\n      cat(\"correlation coefficient between\" , var_names[i], \"and\", var_names[j], \n round(cor_val, 3), \"\\n\")\n    }\n  }\n}\n\n\ncorrelation coefficient between Dist._Green and Dist._CBD -0.93 \ncorrelation coefficient between Top_Univ. and High_School 0.806 \n\n\nDue to multicollinearity, one feature from each pair may need to be dropped.\n\n4.2.1 Feature selection\nTo choose which feature we need to drop in each pair, do the simple decision tree to compare the variable‚Äôs importance level for the dependent variable.\n\nmodel &lt;- train(\n  Property_Prices ~ Dist._Green + Dist._CBD + Top_Univ. + High_School,\n  data = Property_data,\n  method = \"rpart\"\n)\nimportance &lt;- varImp(model)\nprint(importance)\n\nrpart variable importance\n\n            Overall\nDist._CBD    100.00\nTop_Univ.     26.03\nDist._Green   10.67\nHigh_School    0.00\n\n\nNow, drop the least important variable in each pair. We will drop ‚ÄúDist._Green‚Äù and ‚ÄúHigh_School‚Äù.\n\nProperty_data_analysis &lt;- Property_data[, \n  !(names(Property_data) %in% c(\"Dist._Green\", \"High_School\"))\n]\n\nBefore we start building the model, check the data again.\n\nglimpse(Property_data)\n\nRows: 52,644\nColumns: 27\n$ Property_Prices  &lt;dbl&gt; 9.798127, 9.852194, 9.740969, 9.798127, 9.692767, 9.7‚Ä¶\n$ Size             &lt;dbl&gt; 45.0700, 38.1000, 45.0700, 38.1000, 38.1000, 45.0700,‚Ä¶\n$ Floor            &lt;dbl&gt; 8, 13, 6, 13, 7, 9, 6, 6, 11, 7, 9, 9, 10, 11, 2, 4, ‚Ä¶\n$ Highest_floor    &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 11, 11, 1‚Ä¶\n$ Units            &lt;dbl&gt; 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 8, 8, 8, ‚Ä¶\n$ Parking          &lt;dbl&gt; 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67,‚Ä¶\n$ Heating          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ Year             &lt;dbl&gt; 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017,‚Ä¶\n$ Dist._Green      &lt;dbl&gt; 4.668050, 4.668050, 4.668050, 4.668050, 4.668050, 4.6‚Ä¶\n$ Dist._Water      &lt;dbl&gt; 7.092015, 7.092015, 7.092015, 7.092015, 7.092015, 7.0‚Ä¶\n$ Green_Index      &lt;dbl&gt; 10.867812, 10.867812, 10.867812, 10.867812, 10.867812‚Ä¶\n$ Dist._Subway     &lt;dbl&gt; 5.655021, 5.655021, 5.655021, 5.655021, 5.655021, 5.6‚Ä¶\n$ Bus_Stop         &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 13, 13, 13, 13‚Ä¶\n$ Dist._CBD        &lt;dbl&gt; 9.899023, 9.899023, 9.899023, 9.899023, 9.899023, 9.8‚Ä¶\n$ Top_Univ.        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ High_School      &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,‚Ä¶\n$ Sex_Ratio        &lt;dbl&gt; 97.83177, 97.83177, 97.83177, 97.83177, 97.83177, 97.‚Ä¶\n$ Population       &lt;dbl&gt; 31022, 31022, 31022, 31022, 31022, 31022, 31022, 3344‚Ä¶\n$ Pop._Density     &lt;dbl&gt; 1637.045, 1637.045, 1637.045, 1637.045, 1637.045, 163‚Ä¶\n$ Higher_Degree    &lt;dbl&gt; 46.39234, 46.39234, 46.39234, 46.39234, 46.39234, 46.‚Ä¶\n$ Young_Population &lt;dbl&gt; 26.28457, 26.28457, 26.28457, 26.28457, 26.28457, 26.‚Ä¶\n$ Median_Age       &lt;dbl&gt; 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4, 55.4,‚Ä¶\n$ Old_Population   &lt;dbl&gt; 5.712075, 5.712075, 5.712075, 5.712075, 5.712075, 5.7‚Ä¶\n$ Spring           &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,‚Ä¶\n$ Fall             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,‚Ä¶\n$ Winter           &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,‚Ä¶\n$ summer           &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,‚Ä¶"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#building-model",
    "href": "Prototype/Predictive/Take-home_Ex3.html#building-model",
    "title": "Predictive Modeling",
    "section": "5.1 Building model",
    "text": "5.1 Building model\nFirst, plit the dataset into training (80%) and testing (20%) sets. Using 1234 as the seed amount.\nIn this case, Property_Prices is a continuous variable, so method = ‚Äúanova‚Äù is the appropriate choice.\n\nnames(Property_data_analysis) &lt;- gsub(\" \", \"_\", names(Property_data_analysis))\nset.seed(1234)\n\ntrainIndex &lt;- createDataPartition(Property_data_analysis$`Property_Prices`, p = 0.8, \n                                  list = FALSE, \n                                  times = 1)\n\ndf_train &lt;- Property_data_analysis[trainIndex,]\ndf_test &lt;- Property_data_analysis[-trainIndex,]\n\nTrained an initial regression tree using rpart with the following parameters: minsplit = 5, cp = 0.001, and maxdepth = 10. These settings allowed the tree to grow with some constraints to avoid overfitting.\n\nanova.model &lt;- function(min_split, complexity_parameter, max_depth) {\n  rpart(`Property_Prices` ~ ., \n        data = df_train , \n        method = \"anova\", \n        control = rpart.control(minsplit = min_split, \n                                cp = complexity_parameter, \n                                maxdepth = max_depth))\n  }\n\nfit_tree &lt;- anova.model(5, 0.001, 10)\n\n\nvisTree(fit_tree, edgesFontSize = 14, nodesFontSize = 16, width = \"100%\")"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#tuning-of-hyperparameters",
    "href": "Prototype/Predictive/Take-home_Ex3.html#tuning-of-hyperparameters",
    "title": "Predictive Modeling",
    "section": "5.2 Tuning of hyperparameters",
    "text": "5.2 Tuning of hyperparameters\nTo optimize the tree, here we use the complexity parameter (CP) table (cptable) to identify the CP value that minimized the cross-validation error (xerror).\n\nprintcp(fit_tree)\n\n\nRegression tree:\nrpart(formula = Property_Prices ~ ., data = df_train, method = \"anova\", \n    control = rpart.control(minsplit = min_split, cp = complexity_parameter, \n        maxdepth = max_depth))\n\nVariables actually used in tree construction:\n [1] Dist._CBD     Dist._Subway  Green_Index   Heating       Higher_Degree\n [6] Highest_floor Median_Age    Parking       Population    Sex_Ratio    \n[11] Size          Units         Year         \n\nRoot node error: 13564/42117 = 0.32205\n\nn= 42117 \n\n          CP nsplit rel error  xerror      xstd\n1  0.3670796      0   1.00000 1.00007 0.0072552\n2  0.1003165      1   0.63292 0.63376 0.0046167\n3  0.0845250      2   0.53260 0.53303 0.0039374\n4  0.0281388      3   0.44808 0.44856 0.0033575\n5  0.0260674      4   0.41994 0.42040 0.0031686\n6  0.0257959      5   0.39387 0.39161 0.0029997\n7  0.0190744      6   0.36808 0.36859 0.0028409\n8  0.0128092      7   0.34900 0.35003 0.0027295\n9  0.0110859      8   0.33619 0.33717 0.0026830\n10 0.0082922      9   0.32511 0.32613 0.0026348\n11 0.0079784     10   0.31682 0.31867 0.0026345\n12 0.0070377     11   0.30884 0.30873 0.0025974\n13 0.0062992     12   0.30180 0.30143 0.0025070\n14 0.0053719     13   0.29550 0.29666 0.0024660\n15 0.0053494     14   0.29013 0.29334 0.0024275\n16 0.0048172     15   0.28478 0.28723 0.0023723\n17 0.0044387     16   0.27996 0.27925 0.0022879\n18 0.0042845     17   0.27552 0.27605 0.0022688\n19 0.0042792     18   0.27124 0.27358 0.0022510\n20 0.0039115     19   0.26696 0.26885 0.0022247\n21 0.0032249     20   0.26305 0.26187 0.0022090\n22 0.0029020     21   0.25982 0.25732 0.0021862\n23 0.0028710     22   0.25692 0.25509 0.0021776\n24 0.0027361     23   0.25405 0.25000 0.0021617\n25 0.0026756     24   0.25131 0.24830 0.0021576\n26 0.0026218     25   0.24864 0.24690 0.0021617\n27 0.0025135     27   0.24339 0.24636 0.0021602\n28 0.0024512     28   0.24088 0.24443 0.0021524\n29 0.0022157     29   0.23843 0.23954 0.0021279\n30 0.0021056     30   0.23621 0.23869 0.0021166\n31 0.0020036     31   0.23411 0.23586 0.0020999\n32 0.0018931     33   0.23010 0.23215 0.0020393\n33 0.0018549     34   0.22821 0.23027 0.0020330\n34 0.0018325     36   0.22450 0.22786 0.0020233\n35 0.0018306     37   0.22267 0.22708 0.0020129\n36 0.0018065     38   0.22084 0.22524 0.0019517\n37 0.0018002     39   0.21903 0.22513 0.0019519\n38 0.0016511     40   0.21723 0.21993 0.0019180\n39 0.0016419     41   0.21558 0.21705 0.0019060\n40 0.0016414     42   0.21394 0.21705 0.0019060\n41 0.0015938     43   0.21229 0.21488 0.0018984\n42 0.0015292     44   0.21070 0.21240 0.0018866\n43 0.0015192     45   0.20917 0.20883 0.0018702\n44 0.0014909     48   0.20461 0.20662 0.0018579\n45 0.0014809     49   0.20312 0.20324 0.0018475\n46 0.0014746     50   0.20164 0.20312 0.0018401\n47 0.0014727     51   0.20017 0.20266 0.0018385\n48 0.0014677     52   0.19869 0.20237 0.0018365\n49 0.0014513     53   0.19723 0.20158 0.0018291\n50 0.0014251     54   0.19578 0.19997 0.0018185\n51 0.0013960     55   0.19435 0.19798 0.0018134\n52 0.0013757     56   0.19295 0.19575 0.0017979\n53 0.0013575     57   0.19158 0.19469 0.0017889\n54 0.0013406     58   0.19022 0.19306 0.0017787\n55 0.0013342     59   0.18888 0.19159 0.0017520\n56 0.0013215     61   0.18621 0.19038 0.0017474\n57 0.0012404     62   0.18489 0.18765 0.0017291\n58 0.0012187     64   0.18241 0.18417 0.0017182\n59 0.0011913     65   0.18119 0.18311 0.0017111\n60 0.0011332     66   0.18000 0.18082 0.0016998\n61 0.0011152     67   0.17887 0.17901 0.0016876\n62 0.0010916     68   0.17775 0.17727 0.0016730\n63 0.0010737     69   0.17666 0.17685 0.0016702\n64 0.0010453     70   0.17559 0.17576 0.0016629\n65 0.0010384     71   0.17454 0.17373 0.0016532\n66 0.0010367     72   0.17350 0.17315 0.0016506\n67 0.0010057     73   0.17247 0.17256 0.0016486\n68 0.0010000     74   0.17146 0.17141 0.0016420\n\n\n\nbestcp &lt;- fit_tree$cptable[which.min(fit_tree$cptable[,\"xerror\"]),\"CP\"]\npruned_tree &lt;- prune(fit_tree, cp = bestcp)\nvisTree(pruned_tree, edgesFontSize = 14, nodesFontSize = 16, width = \"100%\")"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#model-evaluation",
    "href": "Prototype/Predictive/Take-home_Ex3.html#model-evaluation",
    "title": "Predictive Modeling",
    "section": "5.3 Model Evaluation",
    "text": "5.3 Model Evaluation\n\n\n\n\n\n\nchecking overfitting\n\n\n\nThe training set R-squared (0.8285) is slightly higher than the test set (0.8239), the difference is about 0.47%, which is very small. In terms of MSE, the test set MSE is slightly higher than the training set, but the difference (0.00167) is also small. In conclusion, the performance of the training and test sets is very close, the model has almost no overfitting, and the generalization ability is good.\n\n\nR-squared is only 0.8239, and the model‚Äôs prediction ability is weak. It is recommended to use random forest or further adjust the parameters.\n\n\nshow the code\ntrain_pred &lt;- predict(pruned_tree, newdata = df_train)\ntest_pred &lt;- predict(pruned_tree, newdata = df_test)\n\ntrain_mse &lt;- mean((train_pred - df_train$Property_Prices)^2)\ntest_mse &lt;- mean((test_pred - df_test$Property_Prices)^2)\ntrain_r2 &lt;- 1 - sum((train_pred - df_train$Property_Prices)^2) / \n              sum((df_train$Property_Prices - mean(df_train$Property_Prices))^2)\ntest_r2 &lt;- 1 - sum((test_pred - df_test$Property_Prices)^2) / \n             sum((df_test$Property_Prices - mean(df_test$Property_Prices))^2)\n\ncat(sprintf(\"Train MSE: %.6f\", train_mse))\n\n\nTrain MSE: 0.055219\n\n\nshow the code\ncat(sprintf(\"Test MSE: %.6f\", test_mse))\n\n\nTest MSE: 0.056887\n\n\nshow the code\ncat(sprintf(\"Train R-squared: %.6f\", train_r2))\n\n\nTrain R-squared: 0.828540\n\n\nshow the code\ncat(sprintf(\"Test R-squared: %.6f\", test_r2))\n\n\nTest R-squared: 0.823885\n\n\nThrough the cp table, select an optimal CP value to prune the tree to balance fitting and generalization capabilities. The CP value with the lowest xerror is usually chosen because this indicates that the model performs best on unseen data. ( can be put in shiny)\n\n\nshow the code\ncp_table &lt;- as.data.frame(pruned_tree$cptable)\nnames(cp_table) &lt;- c(\"CP\", \"nsplit\", \"rel_error\", \"xerror\", \"xstd\") \nhead(pruned_tree$cptable, 20)\n\n\n            CP nsplit rel error    xerror        xstd\n1  0.367079585      0 1.0000000 1.0000665 0.007255185\n2  0.100316546      1 0.6329204 0.6337603 0.004616708\n3  0.084525003      2 0.5326039 0.5330338 0.003937360\n4  0.028138831      3 0.4480789 0.4485562 0.003357522\n5  0.026067399      4 0.4199400 0.4203964 0.003168595\n6  0.025795914      5 0.3938726 0.3916060 0.002999738\n7  0.019074431      6 0.3680767 0.3685916 0.002840923\n8  0.012809171      7 0.3490023 0.3500296 0.002729456\n9  0.011085850      8 0.3361931 0.3371698 0.002682996\n10 0.008292161      9 0.3251073 0.3261289 0.002634750\n11 0.007978381     10 0.3168151 0.3186709 0.002634465\n12 0.007037659     11 0.3088367 0.3087277 0.002597356\n13 0.006299227     12 0.3017991 0.3014349 0.002506989\n14 0.005371917     13 0.2954998 0.2966615 0.002466042\n15 0.005349426     14 0.2901279 0.2933445 0.002427479\n16 0.004817197     15 0.2847785 0.2872339 0.002372311\n17 0.004438673     16 0.2799613 0.2792518 0.002287886\n18 0.004284460     17 0.2755226 0.2760482 0.002268796\n19 0.004279165     18 0.2712382 0.2735801 0.002250999\n20 0.003911487     19 0.2669590 0.2688509 0.002224722"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#visualization",
    "href": "Prototype/Predictive/Take-home_Ex3.html#visualization",
    "title": "Predictive Modeling",
    "section": "5.4 Visualization",
    "text": "5.4 Visualization\n\n5.4.1 Predicted vs Actual Plot\nThe points were generally close to the line, confirming the model‚Äôs reasonable predictive performance.\n\n\nshow the code\ntrain_pred &lt;- predict(pruned_tree, newdata = df_train)\ntrain_sse &lt;- sum((train_pred - df_train$Property_Prices)^2)\ntrain_sst &lt;- sum((df_train$Property_Prices - mean(df_train$Property_Prices))^2)\ntrain_r2 &lt;- 1 - train_sse / train_sst\ndf_test$Predicted &lt;- predict(pruned_tree, newdata = df_test)\n\ntree_scatter &lt;- ggplot(df_test, aes(x = Property_Prices, y = Predicted)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +  \n  labs(\n    x = \"Actual Property_Prices\",\n    y = \"Predicted Property_Prices\",\n    title = paste0(\"R-squared (train): \", round(train_r2, 2))\n  ) +\n  theme(\n    axis.text = element_text(size = 5),\n    axis.title = element_text(size = 8),\n    plot.title = element_text(size = 8)\n  )\ntree_scatter\n\n\n\n\n\n\n\n\n\nWe can also compare the actual and predicted data by using boxplot.\n\n\nshow the code\nplot_data &lt;- data.frame(\n  Value = c(df_test$Property_Prices, df_test$Predicted),\n  Type = rep(c(\"Actual\", \"Predicted\"), each = nrow(df_test))\n)\nggplot(plot_data, aes(x = Type, y = Value, fill = Type)) +\n  geom_boxplot() +\n  labs(x = NULL, y = \"Property Prices\", title = \"Boxplot of Actual vs Predicted\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Residual Plot\nThe residual plot showed that residuals were mostly centered around 0, with a slight left skew, indicating that the model occasionally underestimates property prices.\n\n\nshow the code\nresiduals &lt;- test_pred - df_test$Property_Prices\nplot_data &lt;- data.frame(Predicted = test_pred, Residuals = residuals)\nggplot(plot_data, aes(x = Predicted, y = Residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  labs(title = \"Residuals vs Predicted\", x = \"Predicted Property Prices\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n5.4.3 Feature Importance\nA bar plot of feature importance highlighted Size the most influential predictors of property prices, while features like Bus_Stop had small impact.\n\n\nshow the code\nvi &lt;- pruned_tree$variable.importance\nvi_df &lt;- data.frame(\n  Variable = names(vi),\n  Importance = as.numeric(vi)\n)\n\nggplot(vi_df, aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_col(fill = \"lightblue\") +\n  geom_text(aes(label = round(Importance, 2)), hjust = -0.2, size = 3) +  \n  coord_flip() +\n  labs(x = NULL, y = \"Importance\", title = \"Variable Importance from Regression Tree Model\") +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#building-model-1",
    "href": "Prototype/Predictive/Take-home_Ex3.html#building-model-1",
    "title": "Predictive Modeling",
    "section": "6.1 Building Model",
    "text": "6.1 Building Model\nThe default is 8:2 for the training and testing data set. The user can choose the seed number and the ratio of training data.\n\nset.seed(1234)\n\ntrainIndex &lt;- createDataPartition(Property_data_analysis$`Property_Prices`, p = 0.8, list = FALSE,  times = 1)\n\ndf_train &lt;- Property_data_analysis[trainIndex,]\ndf_test &lt;- Property_data_analysis[-trainIndex,]\n\n\n\nfind the best setting\nregisterDoParallel(cores = 4)\n\ntrctrl_none &lt;- trainControl(method = \"none\")\ntrctrl_cv &lt;- trainControl(method = \"cv\", number = 5, verboseIter = TRUE)  trctrl_repeatedcv &lt;- trainControl(method = \"repeatedcv\", number = 5, repeats = 3, verboseIter = TRUE)  \ntrctrl_boot &lt;- trainControl(method = \"boot\", number = 5, verboseIter = TRUE)\n\n\ntune_grid &lt;- expand.grid( mtry = c(2, 4, 8),\n                          min.node.size = c(3, 5, 10), splitrule = c(\"variance\", \"extratrees\") )\n\n\nevaluate_model &lt;- function(tr_control, num_trees, importance_method) \n  { rf_model &lt;- train( Property_Prices ~ ., data = df_train, method = \"ranger\", trControl = tr_control, tuneGrid = tune_grid, num.trees = num_trees, importance = importance_method )\n\n\n  best_params &lt;- rf_model$bestTune cat(\"Cross-validation results:\\n\") \n  print(cv_results) cat(\"Best parameters:\", best_params$mtry, best_params$min.node.size, best_params$splitrule, \"\\n\")\n\npredictions &lt;- predict(rf_model, newdata = df_test) mse &lt;- mean((predictions - df_test$`Property_Prices`)^2)\n  sst &lt;- sum((df_test$Property_Prices - mean(df_test$`Property_Prices`))^2)\n  sse &lt;- sum((predictions - df_test$Property_Prices)^2) r_squared &lt;- 1 - sse / sst cat(\"Test MSE:\", mse, \"\\n\") cat(\"Test R-squared:\", r_squared, \"\\n\")\n\nreturn(list(model = rf_model, cv_results = cv_results, test_mse = mse, test_r_squared = r_squared)) }\n\n\n\ncat(\"\\n=== 5-fold CV, num.trees = 50, importance = impurity ===\\n\") result_cv_50_impurity &lt;- evaluate_model(trctrl_cv, 50, \"impurity\")\n\n\ncat(\"\\n=== Repeated CV (5-fold, 3 repeats), num.trees = 100, importance = permutation ===\\n\") result_repeatedcv_100_permutation &lt;- evaluate_model(trctrl_repeatedcv, 100, \"permutation\")\n\n\ncat(\"\\n=== Bootstrap, num.trees = 200, importance = impurity ===\\n\") result_boot_200_impurity &lt;- evaluate_model(trctrl_boot, 200, \"impurity\")\n\n\nresults_summary &lt;- data.frame( Method = c(\"5-fold CV\", \"Repeated CV (5-fold, 3 repeats)\", \"Bootstrap\"), Num_Trees = c(50, 100, 200), Importance = c(\"impurity\", \"permutation\", \"impurity\"), Test_MSE = c(result_cv_50_impurity$test_mse,\n               result_repeatedcv_100_permutation$test_mse, result_boot_200_impurity$test_mse),\n  Test_R_squared = c(result_cv_50_impurity$test_r_squared, result_repeatedcv_100_permutation$test_r_squared,\n                     result_boot_200_impurity$test_r_squared) ) print(results_summary)\n\nbest_method &lt;- results_summary[which.max(results_summary$Test_R_squared), ] cat(\"\\nBest analysis method:\\n\") print(best_method) '''\n\n\n\n# user can choose to set random seed  (default 1234)\n# user can choose the tuning method :none, cv, repeatedcv, boot(default)\n# user can choose splitrule: variance(default), extratrees, maxstat, beta\n# user can choose num of tree :5-200 (200 is default)\n# user can choose feature importance :\"impurity\", \"permutation\n\ntrctrl &lt;- trainControl(method = \"boot\", number = 5, verboseIter = TRUE)  # number of boot is default\n\n\ntune_grid &lt;- expand.grid(\n  mtry = 8,  # default\n  min.node.size = 3,  # default\n  splitrule = \"variance\"  \n)\n\n\nrf_model &lt;- train(\n  `Property_Prices` ~ ., \n  data = df_train,\n  method = \"ranger\", \n  trControl = trctrl,\n  tuneGrid = tune_grid,\n  num.trees = 200,  \n  importance = \"impurity\" \n)\n\n+ Resample1: mtry=8, min.node.size=3, splitrule=variance \n- Resample1: mtry=8, min.node.size=3, splitrule=variance \n+ Resample2: mtry=8, min.node.size=3, splitrule=variance \n- Resample2: mtry=8, min.node.size=3, splitrule=variance \n+ Resample3: mtry=8, min.node.size=3, splitrule=variance \n- Resample3: mtry=8, min.node.size=3, splitrule=variance \n+ Resample4: mtry=8, min.node.size=3, splitrule=variance \n- Resample4: mtry=8, min.node.size=3, splitrule=variance \n+ Resample5: mtry=8, min.node.size=3, splitrule=variance \n- Resample5: mtry=8, min.node.size=3, splitrule=variance \nAggregating results\nFitting final model on full training set"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#model-evaluation-1",
    "href": "Prototype/Predictive/Take-home_Ex3.html#model-evaluation-1",
    "title": "Predictive Modeling",
    "section": "6.2 Model Evaluation",
    "text": "6.2 Model Evaluation\nThe high R-squared on the test set (0.97) indicates that the random forest model captures the underlying patterns in the data effectively, outperforming the regression tree model (test R-squared of 0.82).\nIf the training set performance (such as R-squared) is much higher than the test set, it may be overfitting. The difference is about 2%, showing slight overfitting, but not a big difference.\n\n\nshow the code\npredictions &lt;- predict(rf_model, newdata = df_test)\nmse &lt;- mean((predictions - df_test$`Property_Prices`)^2)\nr_squared &lt;- 1 - sum((predictions - df_test$`Property_Prices`)^2) / \n                 sum((df_test$`Property_Prices` - mean(df_test$`Property_Prices`))^2)\n\ntrain_predictions &lt;- predict(rf_model, newdata = df_train)\ntrain_mse &lt;- mean((train_predictions - df_train$`Property_Prices`)^2)\ntrain_sst &lt;- sum((df_train$`Property_Prices` - mean(df_train$`Property_Prices`))^2)\ntrain_sse &lt;- sum((train_predictions - df_train$`Property_Prices`)^2)\ntrain_r_squared &lt;- 1 - train_sse / train_sst\n\ncat(\"Test MSE:\", mse)\n\n\nTest MSE: 0.008759057\n\n\nshow the code\ncat(\"Test R-squared:\", r_squared)\n\n\nTest R-squared: 0.9728832\n\n\nshow the code\ncat(\"Train MSE:\", train_mse)\n\n\nTrain MSE: 0.002337131\n\n\nshow the code\ncat(\"Train R-squared:\", train_r_squared)\n\n\nTrain R-squared: 0.992743\n\n\nshow the code\ncat(\"Difference between test and train R-squared:\", train_r_squared - r_squared, \"\\n\")\n\n\nDifference between test and train R-squared: 0.01985979 \n\n\nOOB MSE is the error estimate of unseen data. If it is stable as the number of trees increases, the model has good generalization ability. In this figure, the OOB MSE stabilizes after 50 trees and does not increase significantly, indicating that the model is not overfitted. If it is overfitted, the OOB MSE may start to rise after a certain point.\nThe training set MSE is lower than the OOB and test sets, indicating that the model slightly overfits the training data, but the impact is not significant.\n\n\nshow the code\noob_errors &lt;- sapply(seq(10, 200, by = 10), function(ntree) {\n  model &lt;- ranger(`Property_Prices` ~ ., data = df_train, num.trees = ntree, importance = \"impurity\")\n  return(model$prediction.error)\n})\nplot(seq(10, 200, by = 10), oob_errors, type = \"l\", xlab = \"Number of Trees\", ylab = \"OOB MSE\", main = \"OOB Error vs Number of Trees\")\n\n\n\n\n\n\n\n\n\nshow the code\noob_mse &lt;- rf_model$finalModel$prediction.error\ncat(\"OOB MSE:\", oob_mse, \"\\n\")\n\n\nOOB MSE: 0.009510285"
  },
  {
    "objectID": "Prototype/Predictive/Take-home_Ex3.html#visualization-1",
    "href": "Prototype/Predictive/Take-home_Ex3.html#visualization-1",
    "title": "Predictive Modeling",
    "section": "6.3 Visualization",
    "text": "6.3 Visualization\n\n6.3.1 Predicted vs Actual Plot\nBased on the chart, the points were closely aligned with the line, confirming the model‚Äôs high predictive accuracy (test R-squared of 0.972). The residuals were mostly centered around 0 with minimal spread, indicating that the model‚Äôs predictions were highly accurate and lacked systematic bias.\n\n\nshow the code\ndf_test$Predicted &lt;- predict(rf_model, newdata = df_test)\n\nrf_scatter &lt;- ggplot(df_test, aes(x = Property_Prices, y = Predicted)) +\n  geom_point() +\n  labs(\n    x = \"Actual Property_Prices\",\n    y = \"Predicted Property_Prices\",\n    title = paste0(\"R-squared (train): \", round(rf_model$finalModel$r.squared, 2))\n  ) +\n  theme(\n    axis.text = element_text(size = 5),\n    axis.title = element_text(size = 8),\n    plot.title = element_text(size = 8)\n  )\n\ndf_test$Residuals &lt;- df_test$Predicted - df_test$Property_Prices\n\nrf_residuals &lt;- ggplot(df_test, aes(x = Property_Prices, y = Residuals)) + \n  geom_point(color = \"blue3\") +\n  labs(\n    x = \"Actual Property_Prices\",\n    y = \"Residuals (Predicted - Actual)\"\n  ) + \n  geom_hline(yintercept = 0, color = \"red4\", linetype = \"dashed\", linewidth = 0.5) + \n  theme(\n    axis.text = element_text(size = 5),\n    axis.title = element_text(size = 8)\n  )\n\np &lt;- rf_scatter + rf_residuals +\n  plot_annotation(\n    title = \"Scatterplot of Predicted vs. Actual Property_Prices\",\n    theme = theme(plot.title = element_text(size = 18))\n  )\n\n\np\n\n\n\n\n\n\n\n\n\n\n\n6.3.2 Feature Importance\nThe plot highlighted that Size, Parking, Highest_Floor, Year, and Units were the most influential predictors of property prices, while features like spring had negligible importance (scores close to 0).\n\n\nshow the code\nvi &lt;- varImp(rf_model)\nvi_df &lt;- as.data.frame(vi$importance)\nvi_df$Variable &lt;- rownames(vi_df)\n\nggplot(vi_df, aes(x = reorder(Variable, Overall), y = Overall)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = round(Overall, 2)), hjust = -0.2, size = 3) +  # \n  coord_flip() +\n  labs(x = NULL, y = \"Importance (Overall)\", title = \"Variable Importance from Random Forest Model\") +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes.html",
    "href": "Teams/Minutes/Meeting_Minutes.html",
    "title": "Meeting Minutes",
    "section": "",
    "text": "Project Meeting 1: Project Dataset, Project Proposal and Project Timeline\nProject Meeting 2: Project Proposal\nProject Meeting 3: Project Proposal 2\nProject Meeting 4: Project Poster"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes2.html",
    "href": "Teams/Minutes/Meeting_Minutes2.html",
    "title": "Project Meeting 2: Project Proposal",
    "section": "",
    "text": "Info\n\n\n\nAttendance: Moo Jia Rong, Zhang Xiao Han, Chen Peng-Wei\nDate: 08/03/2025 3.30pm ‚Äì 5.30pm\nMeeting Agenda:\n\nProject Proposal Storyboard Design\nWebsite Design\nProposal, Data Cleaning & Shiny app Task allocations\nFollow-up Action"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-1-project-proposal-storyboard-design",
    "href": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-1-project-proposal-storyboard-design",
    "title": "Project Meeting 2: Project Proposal",
    "section": "Agenda Item 1: Project Proposal Storyboard Design",
    "text": "Agenda Item 1: Project Proposal Storyboard Design\nJia Rong suggested that the team could approach data analyses through segregating into univariate, bivariate and multivariate analyses. Jia Rong suggested doing boxplot and histogram for univariate analysis, scatter plot and correlation heat map for bivariate analysis. Xiao Han suggested combining box plot and smoothed histogram together in one plot and doing radar plot and geospatial plot.¬†\nPeng-Wei suggested adding summary of mean and median values above the plot for the univariate analysis. Xiao Han suggested doing cluster analysis and radar chart for multivariate analysis. Jia Rong suggested adding a parallel coordinate plot to visualize the cluster analysis results.¬†¬†\nPeng-Wei pointed out that our dataset is not very suitable for creating geospatial visualizations as the dataset only has the longitude and latitude coordinates but no region or city data. The team agreed and decided to not focus on geospatial visualizations. Peng Wei also suggested using Canva to draw out the storyboard."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-2-website-design",
    "href": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-2-website-design",
    "title": "Project Meeting 2: Project Proposal",
    "section": "Agenda Item 2: Website Design",
    "text": "Agenda Item 2: Website Design\nXiao Han suggested that we can use the CSS to decorate the website.¬†\nPeng-Wei proposed using basic CSS for the initial design before adding complex features.¬†\nJia Rong recommended adding a navigation bar at the top with clear labels.¬†\nAll agree that the team could develop a website with a green color scheme to match the visualization designs and the overall theme of the project."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-3-proposal-data-cleaning-shiny-app-task-allocations",
    "href": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-3-proposal-data-cleaning-shiny-app-task-allocations",
    "title": "Project Meeting 2: Project Proposal",
    "section": "Agenda Item 3: Proposal, Data Cleaning & Shiny app Task allocations",
    "text": "Agenda Item 3: Proposal, Data Cleaning & Shiny app Task allocations\n\n3.1 Proposal\nFor the proposal, all team members agree to continue working on the proposal, with Jia Rong finishing the write-up, Xiao Han working on the storyboard and Peng-Wei working on the website design.¬†\nTo ensure the quality of proposal all team members agree that doing cross check for each other‚Äôs part of work and then settle down the whole proposal after checking.¬†\n\n\n3.2 Data Cleaning & Shiny app Task allocations\nAll will work on data cleaning to prepare for the project analyses. The submodules will be split amongst the members as follows:¬†\nXiao Han will handle the cluster analysis components of the Shiny app, including implementation of clustering and visualization of parallel coordinate plot.¬†\nJia Rong will focus on bivariate analysis functionality, developing the correlation heatmap and scatterplot components with interactive selection controls.¬†\nPeng-Wei will be responsible for radar chart implementation and univariate analysis features, including summary statistics, boxplots, and distribution visualizations.¬†\nEach team member will handle data cleaning for their respective modules while maintaining consistent data structures for integration."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-4-follow-up-action",
    "href": "Teams/Minutes/Meeting_Minutes2.html#agenda-item-4-follow-up-action",
    "title": "Project Meeting 2: Project Proposal",
    "section": "Agenda Item 4: Follow-up Action",
    "text": "Agenda Item 4: Follow-up Action\nPeng-Wei needs to upload the proposal on the website before 10 March.¬†\nJia Rong will finalize the proposal write-up by 9 March and share with team members for review.¬†\nXiao Han will complete the storyboard designs using Canva by 9 March for team feedback.¬†\nAll team members will review each other‚Äôs work and provide feedback by 10 March to ensure the proposal is ready for submission."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes4.html",
    "href": "Teams/Minutes/Meeting_Minutes4.html",
    "title": "Project Meeting 4: Project Poster",
    "section": "",
    "text": "Info\n\n\n\nAttendance: Moo Jia Rong, Zhang Xiao Han, Chen Peng-Wei\nDate: 29/03/2025 4.00pm ‚Äì 5.00pm\nMeeting Agenda:\n\nDecide Poster Design and Content\nFollow-up Action"
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes4.html#agenda-item-1-decide-poster-design-and-content",
    "href": "Teams/Minutes/Meeting_Minutes4.html#agenda-item-1-decide-poster-design-and-content",
    "title": "Project Meeting 4: Project Poster",
    "section": "Agenda Item 1: Decide Poster Design and Content",
    "text": "Agenda Item 1: Decide Poster Design and Content\nThe team agreed to use green color as the theme for the poster, based on the theme of the project.¬†\nPeng Wei suggested adding the QR code to our shiny app in the poster. Jia Rong thinks it depends on whether we can finish setting up the shiny app website before submission of the poster. Xiaohan suggested we can put QR code linked to group website. Finally, all agree we should link QR code directly to shiny app.¬†\nXiao Han suggested only including the latent class analysis results into the poster for the explanatory modelling module, as there is not enough space to include the multiple regression results. The team agreed to pick the more interesting results to showcase in the poster.¬†\nJia Rong suggested that each team member should work on the content for their own modules in the poster and additional work on one of the following sections: (1) Introduction, (2) brief description of the data set and the modules/ methodology for the project and (3) conclusion/future work."
  },
  {
    "objectID": "Teams/Minutes/Meeting_Minutes4.html#agenda-item-2-follow-up-action",
    "href": "Teams/Minutes/Meeting_Minutes4.html#agenda-item-2-follow-up-action",
    "title": "Project Meeting 4: Project Poster",
    "section": "Agenda Item 2: Follow-up Action",
    "text": "Agenda Item 2: Follow-up Action\nThe group decided on the following deadlines: poster due Tuesday, individual Shiny app due Thursday.¬†\nThe division of poster is as follows:\n(1) Introduction + EDA/CDA =&gt; Jia Rong\n(2) description of the data set and the methodology + predictive modelling=&gt; Peng Wei\n(3) conclusion/future work + Cluster based on the best LCA results =&gt;Xiaohan\nThe group also decided to merge the individual parts after each member completes their assigned section. Since the data preprocessing varies for each part, it was tentatively decided not to merge the data for now.\nThe team agree that we can do our user guideline after the poster presentation if we can‚Äôt finish before the poster presentation."
  }
]